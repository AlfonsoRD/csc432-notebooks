{
 "metadata": {
  "name": "week11-01-optimization"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Optimization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Mathematical optimization is another technique that you will encounter often\n",
      "* We can also develop the solution to least squares as an optimization problem\n",
      "* However, we almost never actually solve least squares as an optimization problem\n",
      "* First, let's discuss optimization in a bit detail"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Let's start with an example\n",
      "* Take some univariate function f(x)\n",
      "* The problem is to find an $x$ such that\n",
      "\n",
      "$$\\min_x f(x)\\mbox{, for }x\\in\\mathbb{R}$$\n",
      "\n",
      "is at a minimum\n",
      "\n",
      "* We will refer to the solution of this problem as $x^*$ \n",
      "* The **arg min** is the point at which $f(\\cdot)$ is at a minimum\n",
      "\n",
      "**Necessary conditions of a local minimum**\n",
      "\n",
      "* Recall from your math classes that there are two necessary conditions for a local minimum \n",
      "\n",
      "   1. $f^{\\prime}(x^*)=0$\n",
      "   2. $f^{\\prime\\prime}(x^*)\\ge 0$\n",
      "    \n",
      "* You might have heard these called **the first-derivative test** and the **second-derivative test**\n",
      "* A *necessary* condition is one that must be true for the conclusion to be true\n",
      "* That is, whenever the conclusion is true (in this case, we have a local minimum) the premise (the first and second derivative conditions given above) must be true\n",
      "* However, the truth of this premise alone is not enough to say we have a local minimum\n",
      "  \n",
      "**Sufficient conditions for a local minimum**\n",
      "   \n",
      "* There are two sufficient conditions for a local minimum\n",
      "\n",
      "   1. $f^{\\prime}(x^*)=0$\n",
      "   2. $f^{\\prime\\prime}(x^*)>0$\n",
      "\n",
      "* A *sufficient condition* is one in which if the premise is true then the conclusion is also true  \n",
      "* So what about when $f^{\\prime}(x^*)=f^{\\prime \\prime}(x^*)=0$?\n",
      "* The sufficient conditions for a local extremum (minimum or maximum) in the case when\n",
      "\n",
      "$f^{\\prime}(x^*)=f^{\\prime \\prime}(x^*)=\\dots=f^{(n-1)}x^*=0$ and $f^{(n)}(x^*)\\neq0$\n",
      "\n",
      "then\n",
      "\n",
      "* If $f^{(n)} > 0$ and $n$ is even, $x^*$ is a local minimum point\n",
      "* If $f^{(n)} < 0$ and $n$ is even, $x^*$ is a local maximum point\n",
      "* If $n$ is odd, then we have a *saddle point* (neither a minimum nor a maximum)\n",
      "* You might have heard this called the **n-th derivative test**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* It is quite common to only speak of minimization problems\n",
      "* In fact, `scipy` only has minimization routines\n",
      "* Why? Any maximization problem can be easily turned into a minimization problem\n",
      "\n",
      "$$\\max_xf(x)=\\min_x-f(x)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "x = np.linspace(-3, 3, 100)\n",
      "\n",
      "ax.plot(x, x**2)\n",
      "ax.set_title(\"$f(x)=-x^2$\")\n",
      "ax.set_ylim(-1, 2)\n",
      "ax.hlines(0, -3, 3)\n",
      "ax.vlines(0, -1, 2)\n",
      "\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.plot(x, -x**2)\n",
      "ax.set_title(\"$f(x)=x^2$\")\n",
      "ax.set_ylim(-2, 1)\n",
      "ax.hlines(0, -3, 3)\n",
      "ax.vlines(0, -2, 1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We worked these in class"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$f(x)=x^2-2$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-3, 3, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "func1 = lambda x : x**2 - 2\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(x, func1(x))\n",
      "ax.set_ylim(-2.5, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$f(x) = x^3 + 4$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "func2 = lambda x : x**3 + 4\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(x, func2(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$f(x)=x^4 - 2x^3 - 2x^2 + 3$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-5, 5, 100)\n",
      "func3 = lambda x : x**4 - 2*x**3 - 2*x**2 + 3\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(x, func3(x))\n",
      "ax.set_ylim(-6, 10)\n",
      "ax.set_xlim(-2, 4);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Using SciPy for Optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(func1, 1.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(func3, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* But be careful of local solutions!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(func3, -2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* There are other optimization methods that may give better results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin_cg(func3, -2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin_bfgs(func3, -2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Luckily (and not by accident) many objective functions used in problems in statistics and machine-learning are [convex functions](http://en.wikipedia.org/wiki/Convex_function) \n",
      "* In these cases, any local extremum is unique and therefore also a global extremum"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Newton's Method for Root-Finding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Above we used a few different optimization algorithms\n",
      "* To fix ideas let's first look at **Newton's method** (also called **Newton-Raphson**)\n",
      "* Newton's method is an algorithm for finding successively better approximations to the root of a function\n",
      "* Consider the following quadratic function\n",
      "\n",
      "$$F(x)=\\frac{1}{4}x^4-\\frac{1}{2}x^2+x$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "func = lambda x : .25*x**4-.5*x**2+x\n",
      "x = np.linspace(-3, 3, 100)\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "\n",
      "ax.plot(x, func(x))\n",
      "#ax.hlines(0, -3, 3)\n",
      "#ax.vlines(0, -4, 10);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* What's the condition for this function to be at a minimum?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* I'm going to call it's derivative $f$ for clarity\n",
      "* So to find the minimum of $F$, we're going to find the root of $f$\n",
      "\n",
      "$$f(x)=x^3-x+1$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "func = lambda x : x**3 - x + 1\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.plot(x, func(x))\n",
      "ax.set_xlim(-2, 2)\n",
      "ax.set_ylim(-4, 4)\n",
      "ax.hlines(0, -2 ,2);\n",
      "ax.vlines(0, -4, 4);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Notice that there is one real root for the derivative\n",
      "* First, we need a good guess for where the root might be\n",
      "* Notice that $f(-2)=-5$ and $f(1)=1$\n",
      "* A sign change indicates that the function passed through zero\n",
      "* So let's choose $x_0=-1$ as our first guess\n",
      "* To perform Newton's method, we need the derivative of the function for which we seek its roots\n",
      "* In our case this is \n",
      "\n",
      "$$f^{\\prime}=3x^2-1$$\n",
      "\n",
      "* We know from the definition of the derivative that the slope of the tangent at this point is\n",
      "\n",
      "$$f^\\prime(x_n)=\\frac{\\Delta y}{\\Delta x}=\\frac{f(x_n)-0}{x_n-x_{n+1}}$$\n",
      "\n",
      "* Re-arranging gives\n",
      "\n",
      "$$x_{n+1}=x_n-\\frac{f(x_n)}{f^\\prime(x_n)}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.plot(x, func(x))\n",
      "ax.plot(x, 2*x+3)\n",
      "ax.plot(-1, 1, 'ko')\n",
      "ax.annotate(\"$x_0$\", (-1,1), (-10,15), textcoords=\"offset points\", fontsize=16)\n",
      "ax.set_xlim(-2, 2)\n",
      "ax.set_ylim(-4, 4)\n",
      "ax.hlines(0, -2 ,2);\n",
      "ax.vlines(0, -4, 4);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We use this to update our guess on $x$ such that\n",
      "\n",
      "$$x_{n+1}= -1 - \\frac{1}{2}$$\n",
      "\n",
      "so that $x_1=-\\frac{1}{2}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.plot(x, func(x))\n",
      "ax.plot(x, 5.75*x+7.75)\n",
      "ax.plot(-1, 1, 'ko')\n",
      "ax.annotate(\"$x_0$\", (-1,1), (-10,15), textcoords=\"offset points\", fontsize=16)\n",
      "ax.plot(-1.5, func(-1.5), 'ko')\n",
      "ax.annotate(\"$x_1$\", (-1.5,func(-1.5)), (10,-5), textcoords=\"offset points\", fontsize=16)\n",
      "ax.set_xlim(-2, 2)\n",
      "ax.set_ylim(-4, 4)\n",
      "ax.hlines(0, -2 ,2);\n",
      "ax.vlines(0, -4, 4);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Update again\n",
      "\n",
      "$$x_{n+1}=x_n-\\frac{f(x_n)}{f^\\prime(x_n)}$$\n",
      "\n",
      "\n",
      "$$x_2=-1.5+\\frac{.875}{5.75}\\approx-1.348$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.plot(x, func(x))\n",
      "ax.plot(-1, 1, 'ko')\n",
      "ax.annotate(\"$x_0$\", (-1,1), (-10,15), textcoords=\"offset points\", fontsize=16)\n",
      "ax.plot(-1.5, func(-1.5), 'ko')\n",
      "ax.annotate(\"$x_1$\", (-1.5,func(-1.5)), (10,-5), textcoords=\"offset points\", fontsize=16)\n",
      "x2 = -1.5+.875/5.75\n",
      "slope = 3*x2**2 - 1\n",
      "intercept = func(x2) - slope*x2\n",
      "ax.plot(x, slope*x + intercept)\n",
      "ax.plot(x2, func(x2), 'ko')\n",
      "ax.annotate(\"$x_1$\", (-1.5, func(-1.5)), (10,-5), textcoords=\"offset points\", fontsize=16)\n",
      "ax.annotate(\"$x_2$\", (x2, func(x2)), (10,-5), textcoords=\"offset points\", fontsize=16)\n",
      "ax.set_xlim(-2, 2)\n",
      "ax.set_ylim(-4, 4)\n",
      "ax.hlines(0, -2 ,2);\n",
      "ax.vlines(0, -4, 4);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Keep iterating until $|x_{n+1}-x_n|<\\epsilon$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "func_prime = lambda x : 3*x**2 - 1\n",
      "\n",
      "x_0 = -1.\n",
      "eps = 1e-8\n",
      "maxiter = 100\n",
      "\n",
      "x_old = x_0\n",
      "change = np.inf\n",
      "for i in range(maxiter):\n",
      "    print i, x_old\n",
      "    if change < eps:\n",
      "        break\n",
      "    x_new = x_old - func(x_old)/func_prime(x_old)\n",
      "    change = np.abs(x_new - x_old)\n",
      "    x_old = x_new\n",
      "else:\n",
      "    print \"Convergence was not achieved in `maxiter` steps\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.newton(func, -1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Multivariate Calculus: A Detour"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* So far we have really only considered univariate functions in detail\n",
      "* *Multivariate* functions are simply functions of more than one variable\n",
      "* In much the same way, we can take derivatives of multivariate functions\n",
      "* Given some function $y = f(x_1, x_2,\\dots,x_n)$, the **partial derivative** of this function is defined in much the same way as the derivative for univariate functions\n",
      "\n",
      "$$\\frac{\\partial y}{\\partial x_i}=\\lim_{\\Delta x_i\\rightarrow 0}\\frac{f(x_1, \\dots, x_i + \\Delta x_i,\\dots,x_n) - f(x_1, \\dots, x_i,\\dots,x_n)}{\\Delta x_i}$$\n",
      "\n",
      "* In fact, the rules for taking the partial derivative of a multivariate function with respect to one of its arguments are exactly the same as for univariate differentiation. You just treat the other arguments of the function as constant.\n",
      "* Consider the following multivariate function\n",
      "\n",
      "$$y=f(x_1, x_2)=3x_1^2 + 2x_1x_2-x_2^2$$\n",
      "\n",
      "* We would take partial derivatives in the following way\n",
      "\n",
      "$$\\frac{\\partial y}{\\partial x_1}=6x_1 + 2x_2$$\n",
      "<br />\n",
      "$$\\frac{\\partial y}{\\partial x_2}=2x_1 - 4x_2$$\n",
      "\n",
      "* The above are often denoted $f_{x_1}$ and $f_{x_2}$ or even $f_1$ and $f_2$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The multivariate analogue of the second derivative is the **second partial derivative**, it maintains the same meaning as in the univariate case\n",
      "* A **cross partial derivative** is a measure of how a partial derivative taken with respect to one argument varies with a very small change in another argument of the function\n",
      "* The second partial derivatives of the example given above are\n",
      "\n",
      "$$\\frac{\\partial^2 y}{\\partial x_1 \\partial x_1}=6$$\n",
      "<br />\n",
      "$$\\frac{\\partial^2 y}{\\partial x_2 \\partial x_2}=-4$$\n",
      "\n",
      "* The cross partial derivatives are\n",
      "\n",
      "$$\\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}=2$$\n",
      "<br />\n",
      "$$\\frac{\\partial^2 y}{\\partial x_2 \\partial x_1}=2$$\n",
      "\n",
      "* Note that $\\frac{\\partial^2 y}{\\partial x_1 \\partial x_2}=\\frac{\\partial^2 y}{\\partial x_2 \\partial x_1}$\n",
      "* This is not a coincidence, [Young's theorem](http://en.wikipedia.org/wiki/Symmetry_of_second_derivatives) states that second derivatives are symmetric with respect to the order of differentiation\n",
      "* Second partial derivatives are often written $f_{x_{1}x_{1}}$ and $f_{x_{2}x_{2}}$ and similarly for cross partials $f_{x_{1}x_{2}}$ and $f_{x_{2}x_{1}}$ or more succinctly as $f_{11}$, $f_{22}$, etc. when the meaning is clear from context"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Vector Calculus: A Detour Part II"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The **gradient** of a multivariate scalar function $f(x_1, x_2, \\dots, x_n)$ at some point $\\boldsymbol{x}=[x_1, x_2, \\dots, x_n]$ is defined to be\n",
      "\n",
      "$$\\nabla F(\\boldsymbol{x})=\\left[\\frac{\\partial F}{\\partial x_1}(\\boldsymbol{x}), \\frac{\\partial F}{\\partial x_2}(\\boldsymbol{x}), \\dots, \\frac{\\partial F}{\\partial x_n}(\\boldsymbol{x})\\right]$$\n",
      "* The **Hessian** of a multivariate function is a matrix of its second partial derivatives\n",
      "\n",
      "$$H(f)=\\left[\\begin{array}{cccc}\n",
      "             \\frac{\\partial^2f }{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2f}{\\partial x_1 \\partial x_n} \\cr\n",
      "             \\frac{\\partial^2f }{\\partial x_2 \\partial x_1} & \\frac{\\partial^2f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2f}{\\partial x_2 \\partial x_n} \\cr\n",
      "             \\vdots & \\vdots & \\ddots & \\vdots \\cr\n",
      "             \\frac{\\partial^2f }{\\partial x_n \\partial x_1} & \\frac{\\partial^2f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2f}{\\partial x_n^2} \\cr\n",
      "\\end{array}\\right]$$\n",
      "* What does Young's theorem tell us about the Hessian?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Optimization of Multivariate Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* With this terminology under our belt, we can define first and second-order conditions for finding the optimum of a multivariate function\n",
      "* If the function\n",
      "\n",
      "$$y = f(x_1, x_2, \\cdots, x_n)=f(\\boldsymbol{x})$$\n",
      "\n",
      "is differentiable with respect to each of its arguments (on a domain) and reaches a maximum or minimum (within the domain)\n",
      "* The first-order condition for the partial derivatives at the extremum $$\\boldsymbol{x}^*=[x^*_1,x^*_2,\\dots,x^*_n]$$ are\n",
      "\n",
      "\n",
      "$$f_1(x^*_1,x^*_2,\\cdots,x^*_n)=0$$\n",
      "$$f_2(x^*_1,x^*_2,\\cdots,x^*_n)=0$$\n",
      "$$\\dots$$\n",
      "$$f_n(x^*_1,x^*_2,\\cdots,x^*_n)=0$$\n",
      "\n",
      "* The second-order conditions states that a sufficient condition for a local maximum is that the Hessian of this function is [negative definite](http://en.wikipedia.org/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices) and for a local minimum the Hessian of this function will be [positive definite](http://en.wikipedia.org/wiki/Positive-definite_matrix)\n",
      "* Stationary points are determined to be saddle points if the Hessian is neither positive or negative definite\n",
      "* We will not discuss these properties of matrices further, though you are encouraged to do so on your own"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Optimization in Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* One place we use optimization a lot is to solve maximum likelihood problems\n",
      "* Maximum likelihood is another way to solve for the parameters of statistical models\n",
      "* Whereas, in least squares, we have the objective function as the sum of squared errors\n",
      "\n",
      "$$\\min_\\beta\\sum_i(y-X\\beta)^2$$\n",
      "\n",
      "* For maximum likelihood, the objective is to maximize what is called the (log-)likelihood function"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Maximum Likelihood"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* If two events $x$ and $y$ are independent, their probabilities do not depend on each other, ie., $cov(x,y)=0$\n",
      "* In this case the joint probability of two events is equal to the product of their individual probabilities\n",
      "* For example, if the probability of a coin flip is $P(tails)=1/2$ and $P(heads)=1/2$, then the probability of observing two heads in a row is $P(heads)\\times P(heads)=(1/2)(1/2)=1/4$\n",
      "* We will use this knowledge to develop what is called the Logit model in econometrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* So far we have assumed that our outcome variables were quantitative in nature\n",
      "* What if the outcome variable were qualititative?\n",
      "  - For example, if we want to predict a woman's decision to participate in the labor force\n",
      "  - a consumer's decision on where to buy a lawnmower\n",
      "  - or the decision of a bank whether or not to make a loan\n",
      "* Say we have some outcome variable, that can only take values of 0 or 1\n",
      "* We could model this using a **linear probability model**\n",
      "$$Pr(y=1|X=x)=x^\\prime\\beta$$\n",
      "\n",
      "* Here, $x$ is one observation\n",
      "* It is *linear* because we have our usual linear combination $X\\beta$ specification on the right hand side\n",
      "\n",
      "$$y = X\\beta+\\epsilon$$\n",
      "\n",
      "* Why wouldn't we want to do this?\n",
      "* Typically least squares depends on the assumption that the residuals of a model are normally distributed conditional on the data\n",
      "\n",
      "$$E[\\epsilon|X]\\sim N(0,\\sigma^2)$$\n",
      "\n",
      "* What does this mean practically and intuitively?\n",
      "* What we are doing in probability models is predicting the mean of $y$ conditional on our data \n",
      "\n",
      "$$E[y|X]=X\\beta$$\n",
      "\n",
      "* When we obtain the residuals $\\epsilon=y-\\hat{y}=y-X\\beta$ we are left with a variable that has an expectation of zero and some spread around it that is often assumed to be Normally distributed (recall that this distribution is often called Gaussian)\n",
      "* Using a similar example to the one from last time"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1234)\n",
      "X = np.random.normal(0, 5, size=1000)\n",
      "\n",
      "X = np.column_stack((np.ones(len(X)), X))\n",
      "beta = np.array([2.5, -2.75])\n",
      "\n",
      "# define y using our data-generating process\n",
      "\n",
      "y = np.dot(X, beta) + np.random.normal(0, 1, size=len(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = np.linalg.lstsq(X, y)[0]\n",
      "print beta \n",
      "\n",
      "errors = y - np.dot(X, beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "\n",
      "ax.hist(errors, bins=13);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    import statsmodels.api as sm\n",
      "    kde = sm.nonparametric.KDEUnivariate(errors)\n",
      "    kde.fit()\n",
      "    \n",
      "    fig, ax = plt.subplots(figsize=(6,6))\n",
      "    ax.plot(kde.support, kde.density)\n",
      "    ax.set_title(\"Distribution of the residuals\")\n",
      "except ImportError: # don't do it if statsmodels isn't installed\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Though the LPM is sometimes used in practice, this assumption is not appropriate if we know $y$ can only take on two values 0 and 1\n",
      "* To avoid certain limitations with the LPM, we use a transformation function $F(\\cdot)$ such that\n",
      "\n",
      "$$P(y=1|x)=F(X\\beta)$$\n",
      "\n",
      "* A popular choice for $F$ is the logistic function we've seen before, which gives us the **logit model**\n",
      "\n",
      "$$F(x)=\\frac{e^x}{1+e^x}$$\n",
      "\n",
      "* Recall that this is the CDF for a logistic random variable"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-3, 3, 100)\n",
      "logistic = lambda x : np.exp(x)/(1+np.exp(x))\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.plot(x, logistic(x)) \n",
      "ax.hlines(0.5, -3, 3, linestyle='--')\n",
      "ax.set_yticks([0, .5, 1]);\n",
      "ax.grid(False)\n",
      "fig.suptitle(r\"$G(x)=\\frac{e^x}{1+e^x}$\", fontsize=20);\n",
      "\n",
      "fig.subplots_adjust(top=.90)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* For such a model, each observation is treated as a single draw from a Bernoulli distribution\n",
      "* Recall that the Bernoulli distribution is the Binomial with one trial - e.g., a single toss of a coin\n",
      "* The success probability is given by $F(x^\\prime\\beta)$, which is the logistic function in the logit model\n",
      "* Assuming the observations are independent gives us the following *likelihood function* for our observed data under the assumption of our model\n",
      "\n",
      "$$P(Y_1=y_1,Y_2=y_2,\\dots,Y_n=y_n)=\\prod_{y_i=0}\\left(1-F(x_i^\\prime\\beta\\right)\\prod_{y_i=1}F(x_i^{\\prime})$$\n",
      "\n",
      "which we can equivalently write as\n",
      "\n",
      "$$L=\\prod_i^nF(x_i^{\\prime}\\beta)^{y_i}(1-F(x_i^{\\prime}\\beta)^{1-y_i}$$\n",
      "\n",
      "* It's usually much easier to work with the log-likelihood\n",
      "* Recall that $log(xy)=\\log(x)+\\log(y)$ and $\\log(x^a)=a\\log(x)$ so that\n",
      "\n",
      "$$\\ln L=\\sum_i^n{y_i}\\ln F(x_i\\beta)+(1-y_i)\\ln[1-F(x_i^{\\prime}\\beta)]$$\n",
      "\n",
      "* Since the logistic function is symmetric, we can simplify\n",
      "\n",
      "$$1-F(x_i^{\\prime}\\beta)=F(-x_i^{\\prime}\\beta)$$\n",
      "\n",
      "* A further simplification results from letting $q=2y-1$ then \n",
      "\n",
      "$$\\ln L=\\sum_iF(q_ix_i\\beta)$$\n",
      "\n",
      "(*Econometric Analysis*, Greene, 21.4)\n",
      "\n",
      "* In general, the likelihood function, usually takes the form\n",
      "\n",
      "$$\\mathcal{L}=\\prod_i f(x_i|\\theta)$$\n",
      "\n",
      "* And we optimize\n",
      "\n",
      "$$\\ln\\mathcal{L}=\\sum_i \\ln f(x_i|\\theta)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logit_func(x):\n",
      "    return np.exp(x)/(1+np.exp(x))\n",
      "\n",
      "def logit_loglikelihood(beta, y, X):\n",
      "    q = 2*y - 1\n",
      "    return -np.sum(np.log(logit_func(q*np.dot(X,beta))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* For example, in 1974, a survey of women only was conducted by *Redbook* asking about extramarital \n",
      "affairs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "\n",
      "data = pandas.read_csv(\"https://raw.github.com/statsmodels/\"\n",
      "                       \"statsmodels/master/statsmodels/datasets/fair/fair.csv\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fair, Ray. 1978. \"A Theory of Extramarital Affairs,\" `Journal of Political\n",
      "    Economy`, February, 45-61.\n",
      "\n",
      "* The original data is available at http://fairmodel.econ.yale.edu/rayfair/pdf/2011b.htm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The original questionnaire asked for time spent per week in an extramarital affair\n",
      "* So let's make affair a *binary variable*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[\"affair\"] = (data[\"affairs\"] > 0).astype(float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[\"const\"] = 1\n",
      "X = data[[\"const\", \"occupation\", \"educ\", \"occupation_husb\", \n",
      "          \"rate_marriage\", \"age\", \"yrs_married\", \n",
      "          \"children\", \"religious\"]]\n",
      "\n",
      "y = data[\"affair\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = X.values # turn into a numpy array\n",
      "y = y.values # turn into a numpy array"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta_0 = np.zeros(9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = optimize.fmin_bfgs(logit_loglikelihood, beta_0, args=(y,X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try: # ok if this isn't installed\n",
      "    from statsmodels.formula.api import logit\n",
      "    affair_mod = logit(\"affair ~ occupation + educ + occupation_husb\" \n",
      "                   \"+ rate_marriage + age + yrs_married + children\"\n",
      "                   \" + religious\", data).fit(disp=0)\n",
      "    print affair_mod.params\n",
      "except ImportError:\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Notice that we did not use either a gradient or a Hessian in this example\n",
      "* I used the BFGS (Broyden-Fletcher-Goldfarb-Shanno) Method\n",
      "* BFGS is a quasi-Newton method\n",
      "* The Hessian is not evaluated directly\n",
      "* The gradient of the objective function is optional, if not supplied it is computed via numerical differentiation\n",
      "* If an optimization method takes an analytic gradient (and Hessian), it is usually a good idea to provide them, if they're not too difficult to solve for"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* For the above example we can solve for the gradient fairly easily\n",
      "* The gradient of a log-likelihood is called a [score function](http://en.wikipedia.org/wiki/Score_(statistics)) in statistics\n",
      "\n",
      "$$\\frac{\\partial \\ln L}{\\partial \\boldsymbol{\\beta}}=\\sum_{i=1}^n\\lambda_ix_i$$\n",
      "\n",
      "where \n",
      "\n",
      "$$\\lambda_i = q_i - F(x^\\prime_i\\beta)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score(beta, y, X):\n",
      "    q = 2*y - 1\n",
      "    L = logit_func(np.dot(X, beta))\n",
      "    return np.dot(y - L, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score(beta, y, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "affair_mod.model.score(affair_mod.params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Convex Optimization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gradient Descent"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Other methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Iterative methods\n",
      "  * Gauss-Newton (sum of squares problems only)\n",
      "  * Levenberg-Marquardt\n",
      "  * Conjugate gradient methods\n",
      "  * Powell's method\n",
      "  * Quasi-Newton methods (BFGS, Broyden)\n",
      "  * Interior point algorithms (constrained optimization)\n",
      "* Global convergence\n",
      "  * Differential evolution\n",
      "  * Genetic algorithms\n",
      "  * Simulated annealing"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}