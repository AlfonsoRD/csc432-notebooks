{
 "metadata": {
  "name": "week10-02-linear_algebra_regression_intro"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linear Algebra Review"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Transposing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that $\\prime$ indicates taking the transpose of a matrix. This just means to swap the rows and the columns of a matrix. So that if \n",
      "\n",
      "$$A = \\left[\\begin{array}{ccc}\n",
      "1 & 2 & 3 \\cr\n",
      "      4 & 5 & 6 \\cr\n",
      "      7 & 8 & 9\\end{array}\\right]$$\n",
      "\n",
      "then\n",
      "\n",
      "$$A^{\\prime} = \\left[\\begin{array}{ccc}\n",
      "      1 & 4 & 7 \\cr\n",
      "      2 & 5 & 8 \\cr\n",
      "      3 & 6 & 9\\end{array}\\right]$$\n",
      "\n",
      "**Properties**\n",
      "\n",
      "$$(A^{\\prime})^{\\prime}=A$$\n",
      "$$(A+B)^{\\prime}=A^{\\prime}+B^{\\prime}$$\n",
      "$$(AB)^{\\prime}=B^{\\prime}A^{\\prime}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Triangular Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A triangular matrix is a square matrix in which all of the elements above or below the diagonal are zero\n",
      "* **Lower** triangular matrix\n",
      "$$L=\\left[\\begin{array}{c c c c c}\n",
      "x_{1,1} & & & & 0 \\cr\n",
      "x_{2,1} & x_{2,2} & & &  \\cr\n",
      "x_{3,1} & x_{3,2} & \\ddots & &  \\cr\n",
      "\\vdots & \\vdots & \\ddots & \\ddots & \\cr\n",
      "x_{n,1} & x_{n,2} & \\cdots & x_{n,n-1} & x_{n,n} \\cr\n",
      "\\end{array}\\right]$$\n",
      "* **Upper** triangular matrix\n",
      "$$U=\\left[\\begin{array}{c c c c c}\n",
      "x_{1,1} & x_{1,2} & \\cdots & x_{1, n-1} & x_{1, n} \\cr\n",
      "& x_{2,2} & x_{2,3} & \\cdots & x_{2,n}  \\cr\n",
      "& & \\ddots & \\ddots & \\vdots \\cr\n",
      "& & & \\ddots & x_{n-1,n} \\cr\n",
      "0 &  & & & x_{n,n} \\cr\n",
      "\\end{array}\\right]$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Multiplication"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Matrix multiplication takes the rows of the matrix that by which you are pre-multiplying and takes linear combination of each row with each column of the matrix by which you are post-multiplying. \n",
      "* To be **conformable** the columns of the $1^{\\text{st}}$ matrix must be equal to the rows of the $2^{\\text{nd}}$ matrix. \n",
      "* The dot product $AB$, $A$ must have the same number of columns as $B$ has rows. \n",
      "* $AB$ has the same number of rows as $A$ and the same number of columns as $B$.  \n",
      "* If A were ($3\\times2$) and B were ($2\\times2$), the result $AB$ would be ($3\\times 2$)\n",
      "* For example,\n",
      "\n",
      "$$A = \\left[\\begin{array}{c c}\n",
      "        2 & 3.5 \\cr\n",
      "        -0.5 & 2.1 \\cr\n",
      "        9 & 0 \\cr\n",
      "\\end{array}\\right]$$\n",
      "\n",
      "and let \n",
      "\n",
      "$$B = \\left[\\begin{array}{c c}\n",
      "            1 & 1.5 \\cr\n",
      "            -4.5 & 5 \\cr\n",
      "\\end{array}\\right]$$\n",
      "\n",
      "Then \n",
      "\n",
      "$$AB=\\left[\\begin{array}{c c}\n",
      "    -13.75 & 20.5 \\cr\n",
      "    -9.95 & 9.75 \\cr\n",
      "        9 & 13.5 \\cr\n",
      "\\end{array}\\right]$$\n",
      "\n",
      "Explicitly, this is\n",
      "\n",
      "$$\\begin{aligned}\n",
      "\\color{blue}{\\text{Row 1, Column 1}} \\cr\n",
      "2\\color{red}{\\times}1 + 3.5\\color{red}{\\times} -4.5 &= -13.75 \\cr\n",
      "\\color{blue}{\\text{Row 1, Column 2}} \\cr\n",
      "2\\color{red}{\\times}1.5 + 3.5\\color{red}{\\times} 5 &= 20.5 \\cr\n",
      "\\color{blue}{\\text{Row 2, Column 1}} \\cr\n",
      "-0.5\\color{red}{\\times}1 + 2.1\\color{red}{\\times} -4.5 &= -9.95 \\cr\n",
      "\\color{blue}{\\text{Row 2, Column 2}} \\cr\n",
      "-0.5 \\color{red}{\\times} 1.5 + 2.1\\color{red}{\\times} 5 &= 9.75 \\cr\n",
      "\\color{blue}{\\text{Row 3, Column 1}} \\cr\n",
      "9 \\color{red}{\\times} 1 + 0 \\color{red}{\\times} -4.5 &= 9 \\cr\n",
      "\\color{blue}{\\text{Row 3, Column 2}} \\cr\n",
      "9 \\color{red}{\\times} 1.5 + 0 \\color{red}{\\times} 5 &= 13.5\n",
      "\\end{aligned}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check with NumPy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[2, 3.5],\n",
      "              [-.5, 2.1],\n",
      "              [9, 0]])\n",
      "\n",
      "B = np.array([[1, 1.5],\n",
      "              [-4.5, 5]])\n",
      "print A\n",
      "print\n",
      "print B\n",
      "print\n",
      "print np.dot(A, B)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Systems of Equations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose you know that a total of `$`175,000 is invested in three assets x, y, and z. <br />\n",
      "The assets pay 6%, 8%, and 10% simple interest, respectively.<br />\n",
      "The yearly interest is $12,950. <br />\n",
      "Twice as much money is invested at 6% than 10%. <br />\n",
      "How much money is invested in the three funds?\n",
      "\n",
      "One way to go about solving this is to first write down a system of equations. We know that the total investment is $175,000 so that\n",
      "\n",
      "    x + y + z = 175,000\n",
      "\n",
      "We know the total yearly interest is \n",
      "\n",
      "    .06x + .08y + .1z = 12,950\n",
      "\n",
      "We also know that twice as much is invested in x as z such that\n",
      "\n",
      "    x = 2z\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Thus we now have 3 equations and 3 unknowns.\n",
      "* We can solve this system using linear algebra\n",
      "* First we write the system in matrix form\n",
      "\n",
      "$$Y=\\left[\\begin{array}{c}\n",
      "          175000 \\cr\n",
      "          12950 \\cr\n",
      "          0\n",
      "\\end{array}\\right]$$\n",
      "\n",
      "$$B = \\left[\\begin{array}{c}\n",
      "        x \\cr\n",
      "        y \\cr\n",
      "        z \\cr\n",
      "\\end{array}\\right]$$\n",
      "\n",
      "$$X=\\left[\\begin{array}{ccc}\n",
      "          1 & 1 & 1 \\cr\n",
      "          .06 & .08 & .1 \\cr\n",
      "          1 & 0 & -2 \\cr\n",
      "\\end{array}\\right]$$\n",
      "\n",
      "Now we have in matrix form\n",
      "\n",
      "$$XB=Y$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Convince yourself that this is the original system of equations.\n",
      "* Suppose, for a minute, that this was a single equation problem in algebra.\n",
      "* How would we solve for $B$?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "The Solution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$B=X^{-1}Y$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Using NumPy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = np.array([[175000.], \n",
      "              [12950], \n",
      "              [0]])\n",
      "\n",
      "X = np.array([[1, 1, 1],\n",
      "              [.06, .08, .1],\n",
      "              [1, 0 , -2]])\n",
      "print\n",
      "print X\n",
      "print\n",
      "print Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "B = np.linalg.solve(X, Y)\n",
      "print B                 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Notice that we did **not** take a matrix inverse here.\n",
      "* Why not?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Matrix Inversion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This is the linear algebra analogue to division\n",
      "* You don't need to concern yourself much with the details for this course\n",
      "* Just remember that you rarely want to do a matrix inversion on a computer\n",
      "* It is more numerically accurate to \"solve\" the system of equations\n",
      "* It takes less arithmetic operations\n",
      "* It is more more memory friendly\n",
      "\n",
      "**Inversion Properties**\n",
      "$$(A^{\\prime})^{-1}=(A^{-1})^{\\prime}$$\n",
      "$$(A^{\\prime})^{\\prime}=A$$\n",
      "$$(kA)^{\\prime}=k^{\\prime}A^{\\prime} \\text{ for nonzero scalar }k$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gaussian Elimination"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* How do we solve these equations without taking an inverse?\n",
      "* One way is **Gaussian elimination**, or **Gauss Jordan elimination**\n",
      "* Example worked in class\n",
      "https://www.youtube.com/watch?v=2j5Ic2V7wq4\n",
      "\n",
      "    $$\\begin{aligned}\n",
      "    x + y - z &= 9 \\cr\n",
      "        y + 3z &= 3 \\cr\n",
      "        -x -2z &= 2 \\cr\n",
      "    \\end{aligned}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "LU Decomposition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* How does a computer solve these equations with taking an inverse\n",
      "* To understand this, we need to understand the concept of a **matrix decomposition**\n",
      "* A **matrix decomposition** is a **factorization** of a matrix into a product of matrices\n",
      "* There are many different decompositions, depending on the problem you are solving\n",
      "* For example, if we have some matrix $X$, we may decompose it into two matrices $A$ and $B$ where\n",
      "\n",
      "$$AB=X$$\n",
      "\n",
      "* By putting certain restrictions on $A$ and $B$ we can use $A$ and $B$ to do things like avoiding inverses"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The decomposition used by `np.linalg.solve` command is the **LU decomposition**\n",
      "* The LU decomposition (or LU factorization)\n",
      "* Let $A$ be a square matrix, then the LU decomposition factors $A$ into two matrices $L$ and $U$ such that\n",
      "\n",
      "$$A=LU$$\n",
      "\n",
      "where $L$ is lower triangular and $U$ is upper triangular\n",
      "\n",
      "* Sometimes we might have to reorder $A$ for the decomposition to work\n",
      "* In this case we have the **LU Factorization with partial pivoting**\n",
      "\n",
      "$$PA=LU$$\n",
      "\n",
      "where $P$ is a permutation matrix that reorders the rows.\n",
      "\n",
      "* For example, the identity matrix is a permutation matrix that does nothing\n",
      "* Using $A$ from our Gaussian elimination example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[1, 1, -1],\n",
      "              [0, 1, 3],\n",
      "              [-1, 0, -2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "P = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
      "print P"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A == np.dot(P, A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Say we want to swap the first and second rows"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "P = np.array([[0, 1, 0],[1, 0, 0],[0, 0, 1]])\n",
      "print P\n",
      "print\n",
      "print A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(P, A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In effect, this says, for the first row of $PA$ give me all the elements in the second column\n",
      "* In the second tow of $PA$ give me all the elements in the first column"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Continuing with the same example, the LU decomposition of $A$ looks like"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import linalg\n",
      "\n",
      "P, L, U = linalg.lu(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print P\n",
      "print\n",
      "print L\n",
      "print\n",
      "print U"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* You'll notice that $U$ is exactly the same as the row-echelon form of $AX=B$ that we saw in our Gaussian elimination example\n",
      "* The computer now does forward elimination just as we did to solve for"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The solution is then \n",
      "\n",
      "$$X = U^{-1}L^{-1}B$$\n",
      "\n",
      "* However, the computer does not need to compute any inverses\n",
      "* Since L and U are triangular matrices\n",
      "* $L^{-1}B$ is computed by forward substitution\n",
      "* Let $C = L^{-1}B$\n",
      "* $U^{-1}C$ is computed using back substitution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "B = np.array([[9],[3],[2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C = np.dot(np.linalg.inv(L), B)\n",
      "print C"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.dot(np.linalg.inv(U), C)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Least Squares Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* What in the world does this have to do with least squares regression?\n",
      "* We are trying to find a $\\boldsymbol{b}$ such that the linear combination of the columns of $X$ gives $\\boldsymbol{y}$\n",
      "\n",
      "$$Y=Xb$$\n",
      "\n",
      "* If $y$ lies in the column space of $X$ then we can solve for $b$ exactly, as we showed above\n",
      "* Recall the usual notation for least squares regression using matrices\n",
      "\n",
      "$$\\boldsymbol{y}=X\\boldsymbol{b}+\\boldsymbol{e}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "A detour into the geometry of Matrices"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Vectors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Note that bold terms are often used to differentiate vectors from scalars much as matrices are often uppercase\n",
      "* The $K$ elements of a column vector\n",
      "\n",
      "$$\\boldsymbol{a} = \\left[\\begin{array}{c}\n",
      "a_1\\cr\n",
      "a_2\\cr\n",
      "\\vdots\\cr\n",
      "a_k\\end{array}\\right]$$\n",
      "\n",
      "can be viewed as the coordinates of a point in a K-dimensional space\n",
      "\n",
      "* Two arithmetic operations are defined for vectors, **scalar multiplication** and **addition**\n",
      "* **Scalar multiplication**\n",
      "\n",
      "$$\\boldsymbol{a} = \\left[\\begin{array}{c}\n",
      "1\\cr\n",
      "2\\end{array}\\right]\\text{,}\\,\\,\\boldsymbol{a}^*=2\\boldsymbol{a}=\\left[\\begin{array}{c}\n",
      "2\\cr\n",
      "4\\end{array}\\right]\\text{,}\\,\\,\\boldsymbol{a}^{**}=-\\frac{1}{2}\\boldsymbol{a}=\\left[\\begin{array}{c}\n",
      "-\\frac{1}{2}\\cr\n",
      "-1\\end{array}\\right]$$\n",
      "\n",
      "* **Addition**\n",
      "$$\\boldsymbol{c}=\\boldsymbol{a}+\\boldsymbol{b}=\\left[\\begin{array}{c}\n",
      "1\\cr\n",
      "2\\end{array}\\right]+\\left[\\begin{array}{c}\n",
      "2\\cr\n",
      "1\\end{array}\\right]=\\left[\\begin{array}{c}\n",
      "3\\cr\n",
      "3\\end{array}\\right]$$\n",
      "* Geometrically"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot([0, 1, 3],[0, 2, 3], color='b')\n",
      "ax.plot([0, 2, 3],[0, 1, 3], color='b')\n",
      "ax.plot([0, 2], [0, 4], color='b')\n",
      "ax.plot([0, 3], [0, 3], color='b')\n",
      "ax.arrow(0, 0, 1, 2, head_width=0.15, head_length=0.1, fc='k', ec='k', length_includes_head=True)\n",
      "ax.arrow(0, 0, 2, 1, head_width=0.15, head_length=0.1, fc='k', ec='k', length_includes_head=True)\n",
      "ax.arrow(0, 0, 3, 3, head_width=0.15, head_length=0.1, fc='k', ec='k', length_includes_head=True)\n",
      "ax.arrow(0, 0, 2, 4, head_width=0.15, head_length=0.1, fc='k', ec='k', length_includes_head=True)\n",
      "ax.annotate(\"$a$\", (.5, 1), (-10, 0), textcoords=\"offset points\", fontsize=16)\n",
      "ax.annotate(\"$a^*$\", (1.5, 3), (-15, 0), textcoords=\"offset points\", fontsize=16)\n",
      "ax.annotate(\"$b$\", (1, .5), (-2,-15), textcoords=\"offset points\", fontsize=16)\n",
      "ax.annotate(\"$c$\", (1.5, 1.5), (-2,-15), textcoords=\"offset points\", fontsize=16)\n",
      "ax.vlines(0, -1, 5)\n",
      "ax.hlines(0, -1, 5)\n",
      "ax.set_xlim(-1, 5)\n",
      "ax.set_ylim(-1, 5);\n",
      "ax.set_title(\"Vector Space - Standard Basis\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "The length of a vector"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The length, or **norm**, of a vector $\\boldsymbol{e}$ is \n",
      "\n",
      "$$\\begin{aligned}\n",
      "\\|e\\| &= \\sqrt{\\boldsymbol{e}^{\\prime}\\boldsymbol{e}} \\cr\n",
      "&= \\sqrt{\\sum_i{e_i^2}} \\cr\n",
      "&= \\sqrt{{e_1^2+e_2^2+e_3^2+\\dots+e_n^2}} \\cr\n",
      "\\end{aligned}$$\n",
      "\n",
      "* where have you seen this before?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can restate the least squares problem in these terms\n",
      "* The problem is to find the $\\boldsymbol{b}$ for which\n",
      "\n",
      "$$\\|\\boldsymbol{e}\\|=\\|y-X\\boldsymbol{b}\\|$$\n",
      "\n",
      "is as small as possible"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Orthogonality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* General term for perpendicular\n",
      "* Two vectors $a$ and $b$ are orthogonal, written $a\\perp b$, if and only if\n",
      "\n",
      "$$a^{\\prime}b=b^{\\prime}a=0$$\n",
      "\n",
      "Let \n",
      "\n",
      "$$a = \\left[\\begin{array}{c}\n",
      "1\\cr\n",
      "3\\end{array}\\right]$$\n",
      "\n",
      "and \n",
      "\n",
      "$$b = \\left[\\begin{array}{c}\n",
      "3\\cr\n",
      "-1\\end{array}\\right]$$\n",
      "                 \n",
      "then\n",
      "\n",
      "$a^{\\prime}b=0$\n",
      "\n",
      "* Geometrically, this would look like"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot([0,1],[0, 3], color='b')\n",
      "ax.plot([0,3],[0,-1], color='b')\n",
      "ax.annotate(\"a\", (.5, 1.5), (-10, 0), textcoords=\"offset points\", fontsize=16)\n",
      "ax.annotate(\"b\", (1.5, -.5), (-2,-15), textcoords=\"offset points\", fontsize=16)\n",
      "ax.arrow(0, 0, 1, 3, head_width=0.15, head_length=0.1, fc='k', ec='k', length_includes_head=True)\n",
      "ax.arrow(0, 0, 3, -1, head_width=0.15, head_length=0.1, fc='k', ec='k', length_includes_head=True)\n",
      "ax.hlines(0, -2, 5)\n",
      "ax.vlines(0, -2, 5)\n",
      "ax.set_xlim(-2, 5)\n",
      "ax.set_ylim(-2, 5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The solution to $Y=X\\boldsymbol{b}+\\boldsymbol{e}$ is the the $\\boldsymbol{b}$ that makes $\\boldsymbol{e}$ perpendicular, or *orthogonal*, to $X\\boldsymbol{b}$\n",
      "* Why?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Solving Least Squares"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The problem then is to find a $b$ such that\n",
      "\n",
      "$$\\boldsymbol{e}\\perp X\\boldsymbol{b}$$\n",
      "\n",
      "* Worked in class\n",
      "* What is the shape of $(X^{\\prime}X)$ if $X$ is ($n\\times k$)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The solution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* As we've seen the solution to the least squares problem is\n",
      "\n",
      "$$\\boldsymbol{b}=\\left(X^{\\prime}X\\right)^{-1}X^{\\prime}\\boldsymbol{y}$$\n",
      "\n",
      "* Recall that we almost never want to take an inverse\n",
      "* There are a few decompositions that are helpful here\n",
      "* First, we need the concept of a **pseudoinverse**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Moore-Penrose Pseudoinverse"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The psuedoinverse is a generalization of the common inverse matrix\n",
      "* Sometimes referred to as the **generalized inverse**\n",
      "* The Moore-Penrose is the most common\n",
      "* The pseudoinverse of $A$ is denoted $A^{+}$\n",
      "\n",
      "**Definition**\n",
      "\n",
      "The pseudoinverse of $A$, $A^{+}$ meets the following four conditions\n",
      "\n",
      "<ol style=\"list-style-type:decimal\">\n",
      "<li>$$AA^{+}A=A$$</li>\n",
      "<li>$$A^{+}AA^{+}=A^{+}$$</li>\n",
      "<li>$$(AA^{+})^{*}=AA^{+}$$</li>\n",
      "<li>$$(A^{+}A)^{*}=A^{+}A$$</li>\n",
      "</ol>\n",
      "\n",
      "\n",
      "**Properties**\n",
      "\n",
      "If $A$ is invertible, then $A^{-1}=A^+$\n",
      "$$(A^+)^+=A$$\n",
      "If the columns of $A$, an ($n \\times k$) matrix, are linearly independent (meaning $n\\ge k$), then\n",
      "$$A^+=(A^\\prime A)^{-1}A^{\\prime}$$\n",
      "\n",
      "Seen anywhere this could come in handy?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First let's simulate some data\n",
      "\n",
      "Suppose we want to fit a model of the form\n",
      "\n",
      "$$y = X\\beta + \\epsilon$$\n",
      "\n",
      "with\n",
      "\n",
      "$$\\epsilon \\sim N(0, 5)$$\n",
      "\n",
      "We are going to use a $\\beta$ of [2.5, -2.75] where 2.5 is the constant."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(12345)\n",
      "X = np.random.normal(0, 5, size=100)\n",
      "\n",
      "X = np.column_stack((np.ones(len(X)), X))\n",
      "beta = np.array([2.5, -2.75])\n",
      "\n",
      "# define y using our data-generating process\n",
      "\n",
      "y = np.dot(X, beta) + np.random.normal(0, 5**.5, size=len(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(7,5))\n",
      "\n",
      "ax.plot(X[:,1], y, 'bo')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = np.linalg.lstsq(X,y)[0]\n",
      "print beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "SVD Decomposition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The Singular Value Decomposition or SVD is one way to solve the least squares problem\n",
      "* Indeed, the pseudoinverse is calculated using the SVD\n",
      "* The SVD of an $m\\times n$ matrix $A$ ($A\\in\\mathbb{R}^{m\\times n}$)\n",
      "\n",
      "$$USV^{*} = A$$\n",
      "\n",
      "* U is an $m\\times m $ unitary matrix\n",
      "* S is an $m\\times n$ diagonal matrix\n",
      "* The elements of S are the singular values (the square root of the eigenvalues) of $A^{*}A$ and $AA^{*}$\n",
      "* $V^{*}$ is an $n\\times n$ matrix which is the conjugate transpose of a unitary matrix $V$\n",
      "* A unitary matrix is any matrix such that\n",
      "\n",
      "$$B^{*}B=BB^{*}=I$$\n",
      "\n",
      "* Recall that if a matrix contains only real numbers then the conjugate transpose is equivalent to the transpose\n",
      "* The real analogue of a unitary matrix is an (orthogonal matrix)\n",
      "\n",
      "$$B^{\\prime}B=BB^{\\prime}=I$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Now that we know what an SVD is, the pseudoinverse of $A$ in terms of the SVD is\n",
      "\n",
      "$$A^{+}=VS^{+}U^{*}$$\n",
      "\n",
      "* Recall that S is an $m\\times n$ diagonal matrix, the pseudoinverse of this is formed by replacing the non-zero diagonal values with their reciprocals and transposing\n",
      "* This is easy to compute\n",
      "* Let's try"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u, s, v = np.linalg.svd(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Huh?\n",
      "* For the routine that numpy uses, s is of shape min($M$, $N$) to save space"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Sinv = np.zeros((2,100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Sinv[:2,:2] = np.diag(1/s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pinvX = np.dot(np.dot(v.T, Sinv), u.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.dot(pinvX, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Or we could just use `np.linalg.pinv`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = np.dot(np.linalg.pinv(X), y)\n",
      "print beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In the best case scenario and assuming $m>n$, which is true for least squares this is $O(mn^2)$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "QR Factorization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Again for $A\\in\\mathbb{R}^{m\\times n}$\n",
      "* The QR factorization is defined\n",
      "\n",
      "$$QR=A$$\n",
      "\n",
      "* Q is an $m\\times m$ orthogonal matrix\n",
      "* R is an $m\\times n$ upper triangular matrix\n",
      "* Since R is upper triangular, you may see\n",
      "\n",
      "$$R = \\left[\\begin{array}{c} R^{\\prime} \\cr\n",
      "       0 \\end{array}\\right]$$\n",
      "where $R^{\\prime}$ is $n\\times n$ upper triangular and 0 is an $(m -n) \\times n$ matrix of zeros"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A helpful property of orthogonal matrices is that they preserve the norm (defined above)\n",
      "* This implies that\n",
      "\n",
      "$$\\|Qe\\|^2=(Qe)^{\\prime}(Qe)=e^{\\prime}Q^{\\prime}Qe=e^{\\prime}Ie=e^{\\prime}e=\\|e\\|^2$$\n",
      "\n",
      "* This follows from properties all defined above\n",
      "* Convince yourself"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can thus apply $Q$ to our residuals $\\|e\\|$\n",
      "* Assuming the system of equations $y=Xb$\n",
      "\n",
      "$\\|e\\|=\\|y-Xb\\|=\\|y-Q\\left[\\begin{array}{c}R \\cr 0\\end{array}\\right]b\\|=\\|Q^{\\prime}y-Q^{\\prime}Q\\left[\\begin{array}{c}R \\cr 0\\end{array}\\right]b\\|=\\|Q^{\\prime}y-\\left[\\begin{array}{c}R \\cr 0\\end{array}\\right]b\\|$\n",
      "\n",
      "If $Q^{\\prime}y=\\left[\\begin{array}{1}c_1 \\cr c_2\\end{array}\\right]$ and $b=\\left[\\begin{array}{1}b_1 \\cr b_2\\end{array}\\right]$ then\n",
      "\n",
      "$$\\|Q^{\\prime}y-\\left[\\begin{array}{c}R \\cr 0\\end{array}\\right]b\\|=\\|\\left[\\begin{array}{1}c_1 \\cr c_2\\end{array}\\right]-\\left[\\begin{array}{c}Rb_1 \\cr 0\\end{array}\\right]\\|=\\|\\left[\\begin{array}{c}c_1 - Rb_1 \\cr c_2\\end{array}\\right]\\|=\\|c_1-Rb_1\\|+\\|c_2\\|$$\n",
      "\n",
      "* Whew, okay\n",
      "* The least squares solution is then determined by solving $Rb_1=c_1$ using back-subsitution and $\\|c_2\\|$ are the residuals"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q, R = np.linalg.qr(X)\n",
      "c_1 = np.dot(Q.T, y)\n",
      "\n",
      "beta = np.linalg.solve(R, c_1)\n",
      "print beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In a stats class, you may see `c_1` referred to as \"effects\"\n",
      "* The QR Factorization is usually $O(n^3)$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}