{
 "metadata": {
  "name": "errors_demo_numdiff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the finite difference approximations of a derivative:\n",
      "\n",
      "$$g_1(\\theta)=f^\\prime(\\theta)\\approx\\frac{f(\\theta+\\delta)-f(\\theta)}{\\delta}$$\n",
      "\n",
      "and \n",
      "\n",
      "$$g_2(\\theta)=f^\\prime(\\theta)\\approx\\frac{f(\\theta+\\delta)-f(\\theta-\\delta)}{2\\delta}$$\n",
      "\n",
      "For holomorphic functions, we can also approximate the derivative \n",
      "\n",
      "$$g_3(\\theta)=f^\\prime(\\theta)\\approx\\frac{Imag[f(\\theta+i\\delta)]}{\\delta}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Numerical approximations to derivatives have two sources of error. \n",
      "* **Truncation error**: Step-size $\\neq$ zero to approximate the limit definition of the derivative\n",
      "  * smaller the step-size, the smaller the truncation error.\n",
      "  * The truncation error is $\\operatorname{O}(\\delta)$ for $g_1$ and $\\operatorname{O}(\\delta^2)$ for $g_2$ and $g_3$.\n",
      "* **Rounding error**: Introduced by smaller and smaller step-size due to the subtraction terms in the approximations  \n",
      "  * The round-off error is $\\operatorname{O}(\\delta^{-1})$\n",
      "  * Note that $g_3(\\theta)$ does not have a subtraction term, and, in fact, we can choose an arbitrarily small step-size for this function, as you can see in the plots below.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How rounding-error occurs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\frac{(1234.1234 - 1234.1236)}{.00001}=\\frac{.2\\times10^{-3}}{.1\\times10^-4}=(\\frac{.2}{.1})(10^{-3}/10^{-4})=20$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ((np.float32(1234.1234) - np.float32(1234.1236))/\n",
      "        np.float32(1e-5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will discuss truncation error in more detail when we deal with Taylor Series and simulation techniques."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Derivative(object):\n",
      "    \"\"\"\n",
      "    Object to evaluate a derivative using forward difference.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    func : function\n",
      "        Function for which you want to evaluate the derivative.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    der : Derivative\n",
      "        A callable object that returns the derivative of `func`\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Based on univariate versions of statsmodels.tools.numdiff\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, func):\n",
      "        self.func = func\n",
      "    \n",
      "    def _approx_fprime(self, x, h=1e-8, args=(), **kwargs):\n",
      "        f = self.func\n",
      "        f_x = f(*((x,)+args), **kwargs)\n",
      "        return (f(*((x+h,)+args), **kwargs) - f_x)/h\n",
      "    \n",
      "    def _approx_fprime_centered(self, x, h=1e-8, args=(), \n",
      "                                      **kwargs):\n",
      "        f = self.func\n",
      "        return (f(*((x+h,)+args), **kwargs) -\n",
      "                f(*((x-h,)+args), **kwargs))/(2*h)\n",
      "    \n",
      "    def _approx_fprime_cs(self, x, h=1e-8, args=(), **kwargs):\n",
      "        f = self.func\n",
      "        ih = h*1j\n",
      "        return f(x+ih, *args, **kwargs).imag / h\n",
      "\n",
      "    \n",
      "    def __call__(self, x, h=1e-8, method=\"fwd\"):\n",
      "        \"\"\"\n",
      "        Return the derivative of self.func using forward difference.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : scalar\n",
      "            The number at which to differentiate.\n",
      "        h : float, optional\n",
      "            The step-size for the forward difference.\n",
      "        method : str {\"fwd\", \"centered\", \"complex\"}\n",
      "            The method of differentiation to use\n",
      "    \n",
      "        Returns\n",
      "        -------\n",
      "        f'(x) : scalar\n",
      "            The derivative of func evaluated at x\n",
      "        \"\"\"\n",
      "        if method == \"fwd\":\n",
      "            return self._approx_fprime(x, h)\n",
      "        \n",
      "        elif method == \"centered\":\n",
      "            return self._approx_fprime_centered(x, h)\n",
      "        \n",
      "        elif method == \"complex\":\n",
      "            return self._approx_fprime_cs(x, h)\n",
      "        \n",
      "        else:\n",
      "            raise ValueError(\"Method not understood\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`__call__` is a special method name for Python classes. You can learn more about the use of special methods [here](http://docs.python.org/2/reference/datamodel.html#special-method-names)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def func(theta):\n",
      "    return 4*np.log(theta) - theta\n",
      "\n",
      "EPS = np.finfo(float).eps # machine epsilon\n",
      "\n",
      "deltas = 10**np.linspace(-20, 0, 1e3)\n",
      "\n",
      "derivative = Derivative(func)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "approx_fprime = [derivative(np.array([5]), eps) \n",
      "                 for eps in deltas]\n",
      "errors = np.abs(-1/5. - np.array(approx_fprime))\n",
      "\n",
      "approx_fprime_cent = [derivative(np.array([5]), eps, \"centered\")\n",
      "                      for eps in deltas]\n",
      "errors_cent = np.abs(-1/5. - np.array(approx_fprime_cent))\n",
      "\n",
      "approx_fprime_cs = [derivative(np.array([5]), eps, \"complex\") \n",
      "                    for eps in deltas]\n",
      "errors_cs = np.abs(-1/5. - np.array(approx_fprime_cs)).squeeze()\n",
      "# some of the errors are below machine precision so we get -inf\n",
      "# this is underflow\n",
      "\n",
      "fig = plt.figure(figsize=(10,20))\n",
      "ax1 = fig.add_subplot(311, xlabel='$\\delta$', \n",
      "            ylabel='|error|', xscale='log', yscale='log');\n",
      "ax1.axvline(EPS, linestyle='--', label='Machine Precision');\n",
      "ax1.legend();\n",
      "ax1.plot(deltas, errors);\n",
      "ax1.set_title(\"Forward Difference\")\n",
      "ax1.annotate(\"Truncation Error\\nDominates\", \n",
      "              (.8e-4, 1e-7), size=\"large\")\n",
      "ax1.annotate(\"Rounding Error\\nDominates\", \n",
      "              (1e-15, 1e-7), size=\"large\")\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(312, xlabel='$\\delta$', \n",
      "            ylabel='|error|', sharex=ax1, sharey=ax1);\n",
      "ax2.axvline(EPS, linestyle='--', label='Machine Precision');\n",
      "ax2.legend();\n",
      "ax2.plot(deltas, errors_cent);\n",
      "ax2.set_title(\"Centered Difference\")\n",
      "ax2.annotate(\"Truncation Error\\nDominates\", \n",
      "              (1e-5, 1e-4), size=\"large\")\n",
      "ax2.annotate(\"Rounding Error\\nDominates\", \n",
      "              (1e-15, 1e-7), size=\"large\")\n",
      "\n",
      "ax3 = fig.add_subplot(313, xlabel='$\\delta$', \n",
      "            ylabel='|error|', sharex=ax1, sharey=ax1);\n",
      "ax3.axvline(EPS, linestyle='--', label='Machine Precision');\n",
      "ax3.legend();\n",
      "ax3.plot(deltas, errors_cs);\n",
      "ax3.set_title(\"Complex Step Derivative\");\n",
      "ax3.annotate(\"Truncation Error\\nDominates\", \n",
      "              (.8e-4, 1e-7), size=\"large\")\n",
      "ax3.annotate(\"No Rounding Error\", \n",
      "              (1e-15, 1e-15), size=\"large\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**References**\n",
      "\n",
      "Ridout, M.S. (2009) \"Statistical Applications of the Complex-Step Method of Numerical Differentiation.\" *The American Statistician,* 63(1), 66-74."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}