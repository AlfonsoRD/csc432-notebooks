{
 "metadata": {
  "name": "",
  "signature": "sha256:f6c5f58befb3e759c265f1230215ca92656dac5da807cfe702e482b386d0e879"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Dimensionality Reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Dimensionality reduction methods, unsurprisingly, try to reduce the number of dimensions in a dataset\n",
      "* Why?\n",
      "  * Curse of dimensionality\n",
      "  * Computational cost\n",
      "  * Reduces noise \n",
      "* Three different ways to accomplish dimensionality reduction\n",
      "  * feature selection (e.g., seeing which features are correlated with output)\n",
      "  * feature derivation (e.g., applying transforms to change the coordinate system)\n",
      "  * clustering"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Curse of Dimensionality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The curse of dimensionality has a few different meanings\n",
      "* In machine learning, we often want to separate data into its different dimensions\n",
      "* As the input space increases you will get fewer and fewer examples in any part of the sample space\n",
      "* Practically, this means that the more features you have, the more samples (observations) you will need and this increases non-linearly"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linear Discriminant Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Linear Discriminant Analysis** is a method of dimensionality reduction credited to Fisher (1936)\n",
      "* The idea is to reduce the dimensionality of the data while preserving as much of the class discriminatory information as possible\n",
      "* Consider the following two groups"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(123)\n",
      "X = np.random.multivariate_normal([0,0], [[.1,0],[0,.1]], size=19)\n",
      "X[:8] -= [1, .25]\n",
      "X[8:] += [1, .25]\n",
      "labels = np.array([0]*8 + [1]*11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Projection Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Let's look at two **projections** of the data given above\n",
      "* Recall that any line can be written as a vector $\\boldsymbol{w}$\n",
      "* The projection of one point $\\boldsymbol{x}$ onto a vector $\\boldsymbol{w}$ can be written $z=\\boldsymbol{w}^\\prime \\boldsymbol{x}$\n",
      "* This is the scalar that is the distance along the vector $\\boldsymbol{w}$ that we need to go to find the projection point"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# arbitrary direction on the line y = 1/2*x\n",
      "w = np.array([[2.], [1.]])\n",
      "print(w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.array([[1.],[2]])\n",
      "print(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(w.T, x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# example worked in class\n",
      "\n",
      "y = lambda x : 1/2.*x\n",
      "xx = np.linspace(-1, 4)\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot(xx, y(xx))\n",
      "ax.plot(x[0], x[1], 'ko')\n",
      "ax.plot([0,x[0]],[0,x[1]], 'k--')\n",
      "\n",
      "ax.hlines(0, -1, 4, 'k')\n",
      "ax.vlines(0, -1, 4, 'k')\n",
      "ax.set_xlim(-1, 4)\n",
      "ax.set_ylim(-1, 4);\n",
      "ax.grid(False);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(w.T, x)/np.dot(w.T, w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point = np.dot(w.T, x)/np.dot(w.T, w) * w\n",
      "print(point)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot(xx, y(xx))\n",
      "ax.plot(x[0], x[1], 'ko')\n",
      "ax.plot([0,x[0]],[0,x[1]], 'k--')\n",
      "ax.plot(point[0], point[1], 'ko')\n",
      "ax.plot([x[0], point[0]], [x[1], point[1]], 'k--')\n",
      "\n",
      "ax.hlines(0, -1, 4, 'k')\n",
      "ax.vlines(0, -1, 4, 'k')\n",
      "ax.set_xlim(-1, 4)\n",
      "ax.set_ylim(-1, 4);\n",
      "ax.grid(False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The following picture shows what happens if we project our dataset onto some vector $\\boldsymbol{w}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "#x = np.linspace(-.25, .25, 50)\n",
      "y = np.linspace(-1.5, 1.5)\n",
      "x = y/-7\n",
      "ax.plot(x, y)\n",
      "ax.set_ylim(-1.5, 1.5)\n",
      "\n",
      "from matplotlib.collections import LineCollection\n",
      "\n",
      "# direction vector for line \n",
      "S = np.array([x[5], y[5]])\n",
      "\n",
      "# orthogonally project all of X1 onto the line\n",
      "X1_proj = (np.dot(X[:8,None],S)/np.dot(S,S)*S)\n",
      "ax.scatter(*X1_proj.T, color='k')\n",
      "\n",
      "lc = LineCollection(zip(X[:8],X1_proj))\n",
      "ax.add_collection(lc)\n",
      "\n",
      "X2_proj = (np.dot(X[8:,None],S)/np.dot(S,S)*S)\n",
      "ax.scatter(*X2_proj.T, color='k')\n",
      "lc2 = LineCollection(zip(X[8:],X2_proj), color='red')\n",
      "ax.add_collection(lc2)\n",
      "\n",
      "\n",
      "ax.annotate(\"w\", (x[-10], y[-10]), (5,0), \n",
      "            textcoords=\"offset points\", fontsize=16)\n",
      "ax.set_title(\"Not linearly separable\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "x = np.linspace(-2, 2, 50)\n",
      "#y = np.linspace(-1.25, -.75, 50)\n",
      "#x = y/.2\n",
      "y = .1*x-1\n",
      "ax.plot(x, y)\n",
      "ax.set_ylim(-1.5, 1.5)\n",
      "ax.set_xlim(-2, 2)\n",
      "\n",
      "#from matplotlib.collections import LineCollection\n",
      "\n",
      "# direction vector for line from origin\n",
      "S = np.array([x[20], y[20]+1])\n",
      "\n",
      "# orthogonally project all of X1 onto the line\n",
      "X1_proj = (np.dot(X[:8,None],S)/np.dot(S,S)*S)\n",
      "# but this line is through the origin, so reflect points\n",
      "X1_proj[:,1] -= 1\n",
      "X1_proj[:,0] += .1\n",
      "ax.scatter(*X1_proj.T, color='k')\n",
      "\n",
      "lc = LineCollection(zip(X[:8],X1_proj))\n",
      "ax.add_collection(lc)\n",
      "\n",
      "X2_proj = (np.dot(X[8:,None],S)/np.dot(S,S)*S)\n",
      "X2_proj[:,1] -= 1\n",
      "X2_proj[:,0] += .1\n",
      "ax.scatter(*X2_proj.T, color='k')\n",
      "lc2 = LineCollection(zip(X[8:],X2_proj), color='red')\n",
      "ax.add_collection(lc2)\n",
      "\n",
      "ax.annotate(\"w\", (x[25],y[25]), (0,5), textcoords=\"offset points\",\n",
      "            fontsize=16)\n",
      "ax.grid(False);\n",
      "ax.set_title(\"Linearly separable\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In order to find a good projection, however, we need to define a measure of separation\n",
      "* First we will compute some statistics of the data\n",
      "* Maybe we could maximize the distance between the projected the means?\n",
      "* This doesn't account for the standard deviation within classes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Fisher's LDA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Fisher proposed maximizing the difference between means, normalized by a measure of within-class scatter\n",
      "* To see how this solution works we need to define some statistics on our dataset\n",
      "* Let's start with means of each class, $\\mu_1$ and $\\mu_2$, the overall mean $\\mu$, and the covariance of each class with itself $\\sum_j(x_j-\\mu)(x_j-\\mu)^{\\prime}$\n",
      "* The principal insight of LDA is what the covariance matrix can tell us about the **scatter** within a dataset\n",
      "  * **scatter** is found by multiplying the covariance by $p_c$ the probability of a point belonging to that class (the number of datapoints in that class / total samples)\n",
      "* Adding the values of this for all the classes gives us a measure of the **within-class scatter**\n",
      "\n",
      "$$S_W=\\sum_c\\sum_{j\\in c}p_c(\\boldsymbol{x}_j-\\boldsymbol{\\mu}_c)(\\boldsymbol{x}_j-\\boldsymbol{\\mu}_c)^{\\prime}$$\n",
      "\n",
      "* If the dataset is easy to separate into classes, then this within-class scatter should be small\n",
      "  * Each class is clustered tightly together\n",
      "* We also want the distance *between* classes to be large, the **between-classes scatter**\n",
      "\n",
      "$$S_B=\\sum_c(\\boldsymbol{u}_c-\\boldsymbol{\\mu})(\\boldsymbol{u}_c-\\boldsymbol{\\mu})^{\\prime}$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "uc_1 = np.mean(X[:8], 0)\n",
      "uc_2 = np.mean(X[8:], 0)\n",
      "center_x = np.r_[uc_1[0], uc_2[0]]\n",
      "center_y = np.r_[uc_1[1], uc_2[1]]\n",
      "\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "ax.plot(uc_1[0], uc_1[1], \"kx\", uc_2[0], uc_2[1], \"kx\", markersize=10)\n",
      "ax.plot(center_x, center_y, \"k\")\n",
      "ax.annotate(\"$S_B$\", (center_x.mean(), center_y.mean()), xytext=(-10, 15),\n",
      "            textcoords=\"offset points\", fontsize=20)\n",
      "\n",
      "slope = np.diff(center_y)/np.diff(center_x)\n",
      "\n",
      "sw1 = 8./19.*np.sum((X[:8] - uc_1)**2)\n",
      "theta = np.arctan(slope)\n",
      "y12 = sw1/2 * np.sin(theta)\n",
      "y11 = -y12\n",
      "x12 = sw1/2 * np.cos(theta)\n",
      "x11 = -x12\n",
      "sw1_x = np.r_[x11,x12]+uc_1[0]\n",
      "# move it below the cluster\n",
      "sw1_y = np.r_[y11-(y12-X[:8,1].min()),+X[:8,1].min()]\n",
      "ax.plot(sw1_x, sw1_y, 'k')\n",
      "ax.annotate(\"$S_{W1}$\", (np.mean(sw1_x),np.mean(sw1_y)),\n",
      "            (-5,-15), textcoords=\"offset points\", \n",
      "            fontsize=16)\n",
      "\n",
      "sw2 = 11./19.*np.sum((X[8:] - uc_2)**2)\n",
      "y22 = sw2/2 * np.sin(theta)\n",
      "y21 = -y22\n",
      "x22 = sw2/2 * np.cos(theta)\n",
      "x21 = -x22\n",
      "sw2_x = np.r_[x21,x22]+uc_2[0]\n",
      "# move it below the cluster\n",
      "sw2_y = np.r_[y21-(y22-X[8:,1].min()),+X[8:,1].min()]\n",
      "ax.plot(sw2_x, sw2_y, 'k')\n",
      "ax.annotate(\"$S_{W2}$\", (np.mean(sw2_x),np.mean(sw2_y)),\n",
      "            (-5,-15), textcoords=\"offset points\", \n",
      "            fontsize=16);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Datasets that are easy to separate into different classes (classes that are **discriminable**) should have $S_B/S_W$ as large as possible.\n",
      "* The between cluster variance should be much larger than the weighted within-cluster variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* What does this tell us about data reduction?\n",
      "* Well we might want to follow the rule that $S_B/S_W$ should be large when reducing the number of dimensions of our data\n",
      "* We can compute a projection of our data onto a vector $\\boldsymbol{w}$ as the linear function $y=w^Tx$\n",
      "* This is Fisher's Linear Discriminant function"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* The mean can be treated like a datapoint such that $\\boldsymbol{\\mu}_c^\\prime=w^T\\cdot\\boldsymbol{\\mu}_c$\n",
      "* The within-class scatter can be computed by replacing $x_j$ with its projection too\n",
      "\n",
      "$$\\sum_c \\sum_{j\\in c}p_c(\\boldsymbol{w}^T\\cdot(\\boldsymbol{x}_j-\\boldsymbol{\\mu}_c))(\\boldsymbol{w}^T\\cdot(\\boldsymbol{x}_j-\\boldsymbol{u}_c))^T=\\boldsymbol{w}^TS_W\\boldsymbol{w}$$\n",
      "\n",
      "* The between-class scatter is\n",
      "\n",
      "$$\\sum_c \\boldsymbol{w}^T(\\boldsymbol{\\mu}_c-\\boldsymbol{\\mu})(\\boldsymbol{\\mu_c}-\\boldsymbol{\\mu})^T\\boldsymbol{w}=\\boldsymbol{w}^TS_B\\boldsymbol{w}$$\n",
      "\n",
      "* Our criterion for discriminability is now \n",
      "\n",
      "$$\\frac{\\boldsymbol{w}^TS_w\\boldsymbol{w}}{\\boldsymbol{w^T}S_B\\boldsymbol{w}}$$\n",
      "\n",
      "* We want to choose the $\\boldsymbol{w}$ that maximizes the separation\n",
      "* To maximize this, we differentiate it and set the derivative equal to 0\n",
      "\n",
      "$$\\frac{S_B\\boldsymbol{w}(\\boldsymbol{w}^TS_w\\boldsymbol{w})-S_W\\boldsymbol{w}(\\boldsymbol{w}^TS_B\\boldsymbol{w})}{(\\boldsymbol{w^T}S_B\\boldsymbol{w})^2}=0$$\n",
      "\n",
      "* We can rearrange this to give\n",
      "\n",
      "$$S_W\\boldsymbol{w}=\\frac{\\boldsymbol{w}^TS_w\\boldsymbol{w}}{\\boldsymbol{w}^TS_B\\boldsymbol{w}}S_B\\boldsymbol{w}$$\n",
      "\n",
      "* Solving for $\\boldsymbol{w}$ requires computing the **generalized eigenvectors** of $S_w^{-1}S_B$\n",
      "* For the two-class case $\\boldsymbol{w}$ is in the direction of $S_W^{-1}(\\mu_1-\\mu_2)$ since the ratio is a scalar it doesn't effect the direction and we can ignore it"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "LDA with Scikit-Learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.lda import LDA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = LDA(n_components=1).fit(X, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can recover our reduced dimension data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn; print(sklearn.__version__)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda.scalings_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# direction vector for line from origin\n",
      "S = lda.scalings_\n",
      "print(S)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Orthogonal projection of the data onto the scalings vector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_proj = np.dot(X, S) / np.dot(S.T,S) * S.T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "ax.set_ylim(-1.5, 1.5)\n",
      "ax.set_xlim(-2, 2)\n",
      "\n",
      "from matplotlib.collections import LineCollection\n",
      "\n",
      "# orthogonally project all of X1 onto the line\n",
      "ax.scatter(*X_proj[:8].T, color='k')\n",
      "\n",
      "lc = LineCollection(zip(X[:8],X1_proj))\n",
      "ax.add_collection(lc)\n",
      "\n",
      "ax.scatter(*X_proj[8:].T, color='k')\n",
      "lc2 = LineCollection(zip(X[8:],X2_proj), color='red')\n",
      "ax.add_collection(lc2)\n",
      "\n",
      "ax.grid(False);\n",
      "ax.set_title(\"LDA Solution\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Fisher developed LDA to deal with the Iris dataset\n",
      "* So let's try that one\n",
      "* What do you get using the Iris data?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "data = iris.data\n",
      "target = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris_lda = LDA(n_components=2).fit(data, target)\n",
      "\n",
      "S = iris_lda.scalings_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(S)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Things to be aware of\n",
      "* LDA is actually maximizing a log-likelihood *assuming* the data is normally distributed and homegenous\n",
      "* It is very similar in practice to multinomial logistic regression "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Principal Components Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* But what if we don't have labelled data?\n",
      "* Or what if we don't want to use the labels?\n",
      "* We can still perform data reduction without using targets\n",
      "* The idea is that we can find a new set of coordinate axes that might make it clear that some dimensions of the data are not needed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1234)\n",
      "X = np.random.multivariate_normal([1,1], [[.05,.1],[.1,.25]], size=40)\n",
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "ax.scatter(*X.T)\n",
      "ax.hlines(0, -.5, 2.5)\n",
      "ax.vlines(0, -.5, 2.5)\n",
      "ax.set_xlim(-.5, 2.5)\n",
      "ax.set_ylim(-.5, 2.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = X - X.mean(0)\n",
      "cov = np.cov(Y, rowvar=0)\n",
      "eigvals, eigvecs = np.linalg.eig(cov)\n",
      "\n",
      "eigvecs *= eigvals**.5\n",
      "dir1, dir2 = eigvecs.T + X.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_mean = X.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "#ax.scatter(*pca.transform(X).T)\n",
      "ax.scatter(*X.T)\n",
      "ax.plot([X_mean[0],dir1[0]], [X_mean[1],dir1[1]], 'k', lw=1.5)\n",
      "ax.plot([X_mean[0],dir2[0]], [X_mean[1],dir2[1]], 'k', lw=1.5)\n",
      "ax.set_xlim(-.5, 2.5)\n",
      "ax.set_ylim(-.5, 2.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The lines can be considered a rotated axes for this dataset\n",
      "* The lengths signify the importance of each dimension\n",
      "* You can see that the \"y\" axis might not be that important and it might be possible to ignore it\n",
      "* But how do we choose the \"important\" axes?\n",
      "* If you peek above, you may get an idea"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* One method is called **Principal Component Analysis**\n",
      "* The basic idea of a **principal component** is that it is a direction in the data with the largest variation\n",
      "* The PCA algorithm centers the data by subtracting the mean, then chooses the direction with largest variation\n",
      "* It places an axis in that direction, then looks at the variation that remains\n",
      "* It finds another axis that is orthogonal to the first and covers as much the remaining variation as possible\n",
      "* This continues until there are no more axes\n",
      "* The end result is a dataset with a diagonal covariance matrix. Why?\n",
      "* The axes found last likely have very little variation and can be removed without affected the variability in the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Aside**: rotation matrices\n",
      "* Rotation matrices are square matrices, with real entries\n",
      "* They are orthogonal and their determinant = 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta = np.deg2rad(45) # 45 degrees in radians\n",
      "\n",
      "# anti-clockwise rotation about the origin\n",
      "P = np.array([[np.cos(theta), -np.sin(theta)],\n",
      "              [np.sin(theta), np.cos(theta)]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point = np.array([[2],\n",
      "                  [1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*point, s=32)\n",
      "ax.plot([0, point[0]], [0, point[1]], 'k--')\n",
      "ax.hlines(0, -1, 3)\n",
      "ax.vlines(0, -1, 3)\n",
      "ax.set_xlim(-1, 3)\n",
      "ax.set_ylim(-1, 3);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can rotate this point by $\\boldsymbol{P}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point2 = np.dot(P, point)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(2, 1, s=32)\n",
      "ax.plot([0, point[0]], [0, point[1]], 'k--')\n",
      "ax.scatter(*point2.T[0], s=32)\n",
      "ax.plot([0, point2[0]], [0, point2[1]], 'k--')\n",
      "ax.hlines(0, -1, 3)\n",
      "ax.vlines(0, -1, 3)\n",
      "ax.set_xlim(-1, 3)\n",
      "ax.set_ylim(-1, 3);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Eigenvectors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Imagine if we have a transformation matrix that reflected points about the line $y=x$\n",
      "* If we had a vector on the line $y=x$, then the reflection of this point is itself\n",
      "* This vector would be an **eigenvector** of that transformation matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[0,1],[1,0]])\n",
      "print(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*point, s=32)\n",
      "ax.scatter(*np.dot(A, point), s=32, color=\"red\")\n",
      "\n",
      "x = np.linspace(0, 3)\n",
      "ax.plot(x, x)\n",
      "ax.margins(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point = np.array([[2],[2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*point, s=32)\n",
      "ax.scatter(*np.dot(A, point), s=32, color=\"red\")\n",
      "\n",
      "x = np.linspace(0, 3)\n",
      "ax.plot(x, x)\n",
      "ax.margins(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Eigenvectors can be found for square matrices only\n",
      "* Though not every square matrix has eigenvectors\n",
      "* Eigenvectors are not unique to any scaling\n",
      "* Scaling a vector only changes its length, not its direction\n",
      "* All eigenvectors of a matrix are orthogonal to each other\n",
      "* As shown above, we will use this fact in PCA to express the original data in terms of the eigenvectors instead of, say, the $x$ and $y$ axes\n",
      "* Eigenvectors are almost always scaled to be of unit length\n",
      "* Take our transformation matrix above\n",
      "* We can find the eigenvectors using numpy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvals, eigvecs = np.linalg.eig(A)\n",
      "print(eigvecs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(np.linalg.norm(eigvecs[:,0]))\n",
      "print(np.linalg.norm(eigvecs[:,1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Eigenvalues"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Take another example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[2,3],[2,1]])\n",
      "print(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point = np.array([[3],[2]])\n",
      "print(point)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(A, point)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, (3, 2) is an eigenvector of $A$ premultipling $A$ by this vector scales the vector by 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(A, point)/point"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This scaling factor $4$ is an **eigenvalue** of $A$ and (3,2) is the associated **eigenvector**\n",
      "* Eigenvalues and eigenvectors always come in pairs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvals, eigvecs = np.linalg.eig(A)\n",
      "print(eigvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "PCA Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Let's look at how this works under the hood before we move on to the derivation\n",
      "* Use the data given above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1234)\n",
      "X = np.random.multivariate_normal([1,1], [[.05,.1],[.1,.25]], size=40)\n",
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "ax.scatter(*X.T)\n",
      "ax.hlines(0, -.5, 2.5)\n",
      "ax.vlines(0, -.5, 2.5)\n",
      "ax.set_xlim(-.5, 2.5)\n",
      "ax.set_ylim(-.5, 2.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, subtract the mean"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Xdemean = X - X.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the covariance matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cov = np.cov(Xdemean, rowvar=False)\n",
      "print(cov)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the eigenvalues and eigenvectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvals, eigvecs = np.linalg.eig(cov)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvecs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvecs *= eigvals**.5\n",
      "X_mean = X.mean(0)\n",
      "dir1, dir2 = eigvecs.T + X_mean\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "\n",
      "ax.scatter(*X.T)\n",
      "ax.plot([X_mean[0],dir1[0]], [X_mean[1],dir1[1]], 'k', lw=1.5)\n",
      "ax.plot([X_mean[0],dir2[0]], [X_mean[1],dir2[1]], 'k', lw=1.5)\n",
      "ax.set_xlim(-.5, 2.5)\n",
      "ax.set_ylim(-.5, 2.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We have found lines that characterize the important features of the data\n",
      "* The next step is transforming our original data in terms of these lines\n",
      "* Let's look at our eigenvalues again"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The largest eigenvalue is called the **Principal component** of the data\n",
      "* It is the direction of the data the \"explains\" the most variance, it is the most significant relationship between the data dimensions\n",
      "* Thus, the next step in PCA is usually to order the eigenvalues in terms of importance\n",
      "* We can use `numpy.argsort` for this"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.argsort(np.array([4,3,2,1,10]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* `argsort` the eigenvalues"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idx = np.argsort(eigvals)[::-1] # reverse the order from largest - smallest\n",
      "print(idx)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvals[idx])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvecs[:,idx])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Now we can choose to ignore the smaller dimensions\n",
      "* The idea is that we lose some information but not much\n",
      "* We can now form a **feature vector** from our eigenvalues"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_vec = eigvecs[:,idx][:,[0]] # keep as 2d vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(feature_vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_data = np.dot(Xdemean, feature_vec)\n",
      "print(new_data[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The data is now expressed solely in terms of the eigenvectors we chose"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* For illustration only, let's also out the data in terms of the second, less important eigenvector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_y = np.dot(Xdemean, eigvecs[:,idx][:,[1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "\n",
      "ax.scatter(new_data, new_y)\n",
      "ax.set_xlim(-.4, .4)\n",
      "ax.set_ylim(-.4, .4);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "PCA Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Formally, say we have a dataset $\\boldsymbol{X}$\n",
      "* We want to rotate it so that the data lies along the directions of maximum variation\n",
      "* To do so multiply $\\boldsymbol{X}$ by a rotation matrix $\\boldsymbol{P}^T$ such that\n",
      "\n",
      "$$\\boldsymbol{Y}=\\boldsymbol{P}^T\\boldsymbol{X}$$\n",
      "\n",
      "where $\\boldsymbol{P}$ is chosen so that the covariance matrix of $\\boldsymbol{Y}$ is diagonal\n",
      "\n",
      "$$\\operatorname{cov}(\\boldsymbol{Y})=\\operatorname{cov}(\\boldsymbol{P}^T\\boldsymbol{X})=\\left(\\begin{array}{ccccc} \\cr\n",
      "\\lambda_1 & 0 & 0 & \\dots & 0 \\cr\n",
      "0 & \\lambda_2 & 0 & \\dots & 0 \\cr\n",
      "\\vdots & \\ddots & \\dots & \\dots & \\dots \\cr\n",
      "0 & 0 & 0 & \\dots & \\lambda_N \\cr\n",
      "\\end{array}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Recall the definition of covariance\n",
      "\n",
      "$$\\operatorname{cov}(Y)=\\mathbb{E}[YY^T]$$\n",
      "\n",
      "* Substitute in our rotated dataset for $Y$\n",
      "\n",
      "$$\\begin{aligned}\\operatorname{cov}(\\boldsymbol{Y})&=\\mathbb{E}[\\boldsymbol{Y}\\boldsymbol{Y}^T] \\cr\n",
      "&=\\mathbb{E}[(\\boldsymbol{P}^T\\boldsymbol{X})(\\boldsymbol{P}^T\\boldsymbol{X})^T] \\cr\n",
      "&=\\mathbb{E}[(\\boldsymbol{P}^T\\boldsymbol{X})(\\boldsymbol{X}^T\\boldsymbol{P})] \\cr\n",
      "&=\\boldsymbol{P}^TE(\\boldsymbol{X}\\boldsymbol{X}^T)\\boldsymbol{P} \\cr\n",
      "&=\\boldsymbol{P}^T\\operatorname{cov}(\\boldsymbol{X})\\boldsymbol{P}\n",
      "\\end{aligned}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This means that\n",
      "\n",
      "$$\\boldsymbol{P}\\operatorname{cov}(\\boldsymbol{Y})=\\boldsymbol{P}\\boldsymbol{P}^T\\operatorname{cov}(\\boldsymbol{X})\\boldsymbol{P}=\\operatorname{cov}(\\boldsymbol{X})\\boldsymbol{P}$$\n",
      "\n",
      "* The second equality follows from the fact that $P^T=P^{-1}$ for rotation matrices\n",
      "* Since the covariance is diagonal we can write $\\boldsymbol{P}$ as a set of column vectors\n",
      "\n",
      "$$\\boldsymbol{P}\\operatorname{cov}(\\boldsymbol{Y})=[\\lambda_1\\boldsymbol{p}_1, \\lambda_2\\boldsymbol{p}_2, \\dots, \\lambda_N\\boldsymbol{p}_N]$$\n",
      "\n",
      "* Let $\\boldsymbol{Z}=\\operatorname{cov}(X)$\n",
      "* And $\\boldsymbol{\\lambda}=\\operatorname{diag}([\\lambda_1, \\lambda_2, \\dots, \\lambda_N])$\n",
      "* We can now write\n",
      "\n",
      "$$\\boldsymbol{P}\\boldsymbol{\\lambda}=\\boldsymbol{Z}\\boldsymbol{P}$$\n",
      "\n",
      "or\n",
      "\n",
      "$$\\boldsymbol{\\lambda}=\\boldsymbol{P}^{-1}\\boldsymbol{Z}\\boldsymbol{P}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $\\boldsymbol{\\lambda}$ is a diagonal matrix, so all it does is rescale the rotation matrix $\\boldsymbol{P}$\n",
      "* This means we have found a matrix $P$ such that "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $\\boldsymbol{\\lambda}$ are the **eigenvalues** of $\\boldsymbol{Z}$\n",
      "* The eigenvalues of a matrix $\\boldsymbol{Z}$ are the roots to the *characteristic equation*, or *characteristic polynomial*\n",
      "\n",
      "    $$determinant(\\boldsymbol{Z}-\\boldsymbol{\\lambda}I)=|\\boldsymbol{Z}-\\boldsymbol{\\lambda}I|=0$$\n",
      "    </br>\n",
      "* Let $\\lambda_1$ be an eigenvalue of $Z$, then there is a vector $\\boldsymbol{P_1}$ such that\n",
      "\n",
      "$$\\lambda\\boldsymbol{P_1}=\\boldsymbol{Z}\\boldsymbol{P_1}$$\n",
      "\n",
      "* The vector $\\boldsymbol{P_1}$ is the **eigenvector** of $\\boldsymbol{Z}$ associated with the eigenvalue $\\lambda_1$\n",
      "* Notice that $\\boldsymbol{P_1}$ is not unique. It is only a direction vector and can have arbitrary scaling.\n",
      "* As mentioned above, it is common to scale all of the eigenvectors of a matrix to have unit length"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "An Example using NumPy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z = np.array([[ 3., 0., 1.],\n",
      "              [4., 6., 2.],\n",
      "              [3., 1., 2.]]) \n",
      "\n",
      "eigvals, eigvecs = np.linalg.eig(Z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvecs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Eigenvectors are in the columns\n",
      "* Unit length"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(eigvecs**2, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(eigvecs, np.diag(eigvals))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(Z, eigvecs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first eigenvalue"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(eigvecs[:,0], np.dot(Z, eigvecs[:,0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "PCA Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Takes some data $\\boldsymbol{X}$\n",
      "* Subtract the mean\n",
      "* Get the covariance matrix $\\operatorname{cov}(\\boldsymbol{X})=\\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}$\n",
      "* Calculate the eigenvalues and eigenvectors\n",
      "* Sort the eigenvectors in *decreasing* order of eigenvalues\n",
      "* Ignore eigenvectors with an associated eigenvalue less than some $\\epsilon$, leaving $L$ dimensions in the data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Another Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(12345)\n",
      "\n",
      "cov = [[.4,2],[2,2]]\n",
      "\n",
      "rvs = np.random.multivariate_normal([6,-1], cov, size=500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*rvs.T, alpha=.6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Using NumPy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rvs_demean = rvs - rvs.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(np.cov(rvs_demean, rowvar=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvals, eigvecs = np.linalg.eig(cov)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(eigvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idx = np.argsort(eigvals)[::-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvecs[:,idx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Take only the largest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_vec = eigvecs[:,idx][:,[0]] # preserve 2d\n",
      "print(feature_vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Project the data onto the eigenvectors and translate back to mean"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.dot(rvs_demean, feature_vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, axes = plt.subplots(1, 2, figsize=(12,6), sharex=True, sharey=True)\n",
      "\n",
      "axes[0].scatter(*rvs.T, alpha=.6)\n",
      "axes[1].scatter(x, [0]*len(x), alpha=.6);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Using scikit-learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(rvs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.n_components = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reduce the dimension"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = pca.fit_transform(rvs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reduced data in the direction of the principal component"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_direc = np.dot(y, pca.components_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, axes = plt.subplots(1, 2, figsize=(12,6), sharex=True, sharey=True)\n",
      "\n",
      "axes[0].scatter(*rvs.T, alpha=.6)\n",
      "axes[1].scatter(*y_direc.T, alpha=.6);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Iris dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Do the same for the Iris dataset\n",
      "* Reduce the number of dimensions to 2 using PCA\n",
      "* Plot the data in two dimensions, using colors to differentiate between classes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Kernel PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A potential problem with PCA is that it assumes directions of variation are all straight lines\n",
      "* We will extend PCA using the **kernel trick**\n",
      "* The kernel trick involves apply a (possibly non-linear) functin $\\Phi(\\cdot)$ to each datapoint $\\boldsymbol{x}$\n",
      "* This transforms the data into \"kernel space\" then we perform linear PCA in that space"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* As we would expect, the covariance defined in the kernel space is \n",
      "\n",
      "$$\\boldsymbol{C}=\\frac{1}{N}\\sum_{n=1}^N\\Phi(x_n)\\Phi(x_n)^T$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This produces the eigenvector equation\n",
      "\n",
      "$$\\lambda(\\Phi(\\boldsymbol{x}_i)\\boldsymbol{V})=(\\Phi(X_i)\\boldsymbol{C}\\boldsymbol{V})$$\n",
      "\n",
      "for $i=1,\\dots,N$\n",
      "\n",
      "* where $V=\\sum{_j=1}^N\\alpha_j\\Phi(X_j)$ are the eigenvectors of the original problem \n",
      "* the $\\alpha_j$ are the eigenvectors of the 'kernelized' problem\n",
      "* We can apply the kernel trick to produce an $N\\times N$ matrix $\\boldsymbol{K}$\n",
      "\n",
      "$$K_{ij}=\\Phi(\\boldsymbol{x}_i\\boldsymbol{x}_j)$$\n",
      "          \n",
      "* Combining the two gives\n",
      "\n",
      "$$V^k\\Phi(\\boldsymbol{x})=\\sum_{i=1}^N \\boldsymbol{\\alpha}_i^k(\\Phi(x_i)\\Phi(x_j))$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Iris Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "data = iris.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import KernelPCA\n",
      "\n",
      "iris_kpca = KernelPCA(kernel=\"rbf\").fit(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_data = iris_kpca.transform(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(new_data[:50, 0], new_data[:50, 1], color='red', s=32)\n",
      "ax.scatter(new_data[50:100, 0], new_data[50:100,1], color='green', s=32)\n",
      "ax.scatter(new_data[100:, 0], new_data[100:, 1], color='blue', s=32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Another Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_circles\n",
      "\n",
      "np.random.seed(0)\n",
      "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
      "\n",
      "reds = y == 0\n",
      "blues = y == 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.set_title(\"Original space\")\n",
      "\n",
      "ax.plot(X[reds, 0], X[reds, 1], \"ro\")\n",
      "ax.plot(X[blues, 0], X[blues, 1], \"bo\")\n",
      "ax.set_xlabel(\"$x_1$\")\n",
      "ax.set_ylabel(\"$x_2$\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* PCA fails to linearly separate this data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_pca = PCA().fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.set_title(\"Projection using PCA\")\n",
      "\n",
      "ax.plot(X_pca[reds, 0], X_pca[reds, 1], \"ro\")\n",
      "ax.plot(X_pca[blues, 0], X_pca[blues, 1], \"bo\")\n",
      "ax.set_xlabel(\"First principal component\")\n",
      "ax.set_ylabel(\"Second principal component\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Kernel PCA, on the other hand"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
      "X_kpca = kpca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6), subplot_kw=dict(aspect='equal'))\n",
      "\n",
      "ax.set_title(\"Projection using KernelPCA\")\n",
      "\n",
      "ax.plot(X_kpca[reds, 0], X_kpca[reds, 1], \"ro\")\n",
      "ax.plot(X_kpca[blues, 0], X_kpca[blues, 1], \"bo\")\n",
      "ax.set_xlabel(\"1st principal comp. in space induced by $\\Phi$\")\n",
      "ax.set_ylabel(\"2nd principal comp. in space induced by $\\Phi$\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Factor Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In factor analysis, we want to know if observed data can be explained by a smaller number of unobserved, uncorrelated **factors**, or **latent variables**\n",
      "* The goal of factor analysis is to uncover the independent factors and the noise inherent in their measurement\n",
      "* Factor analysis is used often in psychology, social sciences, and finance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Suppose we have a dataset $\\boldsymbol{X}$ that is $N\\times M$ and has a covariance matrix $\\boldsymbol{\\Sigma}$\n",
      "* Assume that $\\boldsymbol{X}$ is centered such that each column has mean zero\n",
      "* Ie., for each column $j$ we have $\\boldsymbol{b}_j=\\boldsymbol{x}_j-\\boldsymbol{\\mu}_j$\n",
      "* We can write the assumed model as\n",
      "\n",
      "$$\\boldsymbol{X}=\\boldsymbol{W}\\boldsymbol{b}+\\boldsymbol{\\epsilon}$$\n",
      "\n",
      "where \n",
      "\n",
      "* $\\boldsymbol{X}$ is the data and $\\boldsymbol{\\epsilon}$ is the noise\n",
      "* $\\boldsymbol{b}$ are the latent factors such that it's a matrix of column variables $\\boldsymbol{b}=[\\boldsymbol{b}_1, \\boldsymbol{b}_2, \\dots, \\boldsymbol{b}_k]$\n",
      "* and $\\boldsymbol{W}$ are called the **factor loadings**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can now rewrite the covariance matrix of $\\boldsymbol{X}$ as\n",
      "\n",
      "$$\\begin{aligned}\\operatorname{cov}(X)&=\\operatorname{cov}(\\boldsymbol{W}\\boldsymbol{b}+\\boldsymbol{\\epsilon}) \\cr\n",
      "&=\\operatorname{cov}(\\boldsymbol{W}\\boldsymbol{b})+\\operatorname{cov}(\\boldsymbol{\\epsilon}) \\cr\n",
      "&=\\boldsymbol{W}\\boldsymbol{W}^T+\\Psi\n",
      "\\end{aligned}$$\n",
      "\n",
      "* The third line follows from the fact that $\\operatorname{cov}(\\boldsymbol{b})=\\boldsymbol{I}$ because the factors are uncorrelated\n",
      "* Assuming the data is demeaned, then\n",
      "\n",
      "$$X \\sim \\mathcal{N}(0, WW^T + \\Psi)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* For Factor Analysis, we need one more assumption on the structure of the error covariance $\\boldsymbol{\\Psi}$\n",
      "* We will assume that $\\boldsymbol{\\Psi}=\\operatorname{diag}({\\psi_1, \\psi_2, \\dots, \\psi_m})$\n",
      "* A related model, called Probabilistic PCA, assumes that $\\boldsymbol{\\Psi}=\\sigma^2 \\boldsymbol{I})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Since we have a distribution, to estimate both $\\boldsymbol{W}$ and $\\boldsymbol{\\Psi}$ we would write down the likelihood of the model\n",
      "* We can maximize this likelihood by using a technique called Expectations Maximization or the EM algorithm\n",
      "* We won't discuss this algorithm for now"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Image Processing Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "from numpy.random import RandomState\n",
      "\n",
      "n_row, n_col = 3, 3\n",
      "n_components = n_row * n_col\n",
      "image_shape = (64, 64)\n",
      "rng = RandomState(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try: # 2/3 bug\n",
      "    dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
      "    faces = dataset.data\n",
      "except: # on python 2.6/2.7\n",
      "    # caching\n",
      "    from os.path import exists, join\n",
      "    from sklearn.base import get_data_home\n",
      "    from sklearn.externals import joblib\n",
      "    data_home = get_data_home(data_home=None)\n",
      "    TARGET_FILENAME = \"olivetti.pkz\"\n",
      "    if not exists(join(data_home, TARGET_FILENAME)):\n",
      "        \n",
      "        from urllib2 import urlopen\n",
      "        from StringIO import StringIO as BytesIO\n",
      "        from scipy.io import loadmat\n",
      "        DATA_URL = \"http://cs.nyu.edu/~roweis/data/olivettifaces.mat\"\n",
      "        fhandle = urlopen(DATA_URL)\n",
      "        buf = BytesIO(fhandle.read())\n",
      "        mfile = loadmat(buf)\n",
      "        faces = mfile['faces'].T.copy()\n",
      "        joblib.dump(faces, join(data_home, TARGET_FILENAME), compress=6)\n",
      "    else:\n",
      "        faces = joblib.load(join(data_home, TARGET_FILENAME))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples, n_features = faces.shape\n",
      "\n",
      "faces_centered = faces - faces.mean(axis=0)\n",
      "\n",
      "# local centering\n",
      "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
      "\n",
      "print(\"Dataset consists of %d faces\" % n_samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
      "    fig, axes = plt.subplots(n_row, n_col, figsize=(2. * n_col, 2.26 * n_row),\n",
      "                             squeeze=False)\n",
      "    fig.suptitle(title, size=16)\n",
      "    for i, comp in enumerate(images):\n",
      "        ax = axes[i // n_col, i % n_col]\n",
      "        vmax = max(comp.max(), -comp.min())\n",
      "        ax.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
      "                  interpolation='nearest',\n",
      "                  vmin=-vmax, vmax=vmax)\n",
      "        ax.set_xticks(())\n",
      "        ax.set_yticks(())\n",
      "    fig.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import FactorAnalysis\n",
      "\n",
      "factor_analysis = FactorAnalysis(n_components=n_components, max_iter=2)\n",
      "\n",
      "factor_analysis.fit(faces_centered)\n",
      "\n",
      "components = factor_analysis.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_gallery(\"Factor Analysis Decomposition - %d components\" % n_components, components[:n_components])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Independent Component Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Independent Component Analysis** or **ICA** is another approach to factor analysis\n",
      "* With PCA we required that the components were orthogonal and **uncorrelated** (a diagonal covariance matrix)\n",
      "* A diagonal covariance matrix implies that variables are uncorrelated (a linear measure) but NOT that they are independent\n",
      "* Though the converse is true, independent variables will have a diagonal covariance matrix\n",
      "* ICA forces the components to also be statistically independent\n",
      "* Statistical independence is defined such that that $\\mathbb{E}[b_i]\\mathbb{E}[b_j]=\\mathbb[b_i,b_j]$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A very common use of ICA is **blind source separation**\n",
      "* As with factor analysis, we assume that the data we see are created by underlying physical processes that are independent\n",
      "* However, the data we see are correlated\n",
      "* This can be due to a mixing of the different processes\n",
      "* A common example is the **cocktail party problem** \n",
      "* Even though there are many sounds (conversations, background music, glasses), we are able to discern single conversations\n",
      "* Unlike humans, the ICA algorithm requires that there are as many \"ears\" as there are signal sources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Assume we have two sources making noise over time ($s_1^t$,$s_2^t$)\n",
      "* We have two microphones picking up both signals ($x_1^t$,$x_2^t$)\n",
      "* The sounds that are heard are\n",
      "\n",
      "$$x_1 = as_1 + bs_2$$\n",
      "$$x_2 = cs_1 + ds_2$$\n",
      "\n",
      "or in matrix form\n",
      "\n",
      "$$\\boldsymbol{x}=\\boldsymbol{A}\\boldsymbol{s}$$\n",
      "\n",
      "where $\\boldsymbol{A}$ is the **mixing matrix**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We need to uncover the **mixing matrix** to solve this\n",
      "* To do so, we use the following three facts\n",
      "\n",
      "1. the mixtures are not independent even though the sources are\n",
      "2. the mixtures will look like normal distributions even if the sources don't (the central limit theorem again)\n",
      "3. the mixtures are more complicated than the sources\n",
      "\n",
      "* These facts allow us to uncover the mixing matrix, and, therefore, the source signals using insights from *information theory*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "An Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Generate some example data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(0)\n",
      "n_samples = 2000\n",
      "time = np.linspace(0, 10, n_samples)\n",
      "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
      "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
      "S = np.c_[s1, s2]\n",
      "S += 0.2 * np.random.normal(size=S.shape)  # Add noise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Standardize the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "S /= S.std(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(10,5))\n",
      "\n",
      "ax.plot(S)\n",
      "ax.set_title(\"True Sources\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Mix the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([[1, 1], [0.5, 2]])  # Mixing matrix\n",
      "X = np.dot(S, A.T)  # Generate observations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(10,5))\n",
      "\n",
      "ax.plot(X)\n",
      "ax.set_title(\"Observed Signal\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Compute ICA using the FastICA implementation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ica.mixing_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import FastICA\n",
      "\n",
      "ica = FastICA()\n",
      "S_ = ica.fit(X).transform(X)  # Get the estimated sources\n",
      "A_ = ica.mixing_  # Get estimated mixing matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(10,5))\n",
      "\n",
      "ax.plot(S_)\n",
      "ax.set_title(\"Recovered Sources\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "An Example using Stock Prices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Note Factor Analysis and PCA are actually not that useful for discovering market structure in pure returns\n",
      "* There is no inherent correlation structure\n",
      "* The first component is like the entire market moving with trends (something like $\\beta$ or CAP-M)\n",
      "* You'll see them more often in Yield Curve Models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.io.data import get_components_yahoo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.io.data import DataReader"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datetime.today() - pandas.offsets.Day(1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = datetime.today() - pandas.offsets.Day(365)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "symbol_dict = {\n",
      "    'TOT': 'Total',\n",
      "    'XOM': 'Exxon',\n",
      "    'CVX': 'Chevron',\n",
      "    'COP': 'ConocoPhillips',\n",
      "    'VLO': 'Valero Energy',\n",
      "    'MSFT': 'Microsoft',\n",
      "    'IBM': 'IBM',\n",
      "    'TWX': 'Time Warner',\n",
      "    'CMCSA': 'Comcast',\n",
      "    'CVC': 'Cablevision',\n",
      "    'YHOO': 'Yahoo',\n",
      "    'DELL': 'Dell',\n",
      "    'HPQ': 'HP',\n",
      "    'AMZN': 'Amazon',\n",
      "    'TM': 'Toyota',\n",
      "    'CAJ': 'Canon',\n",
      "    'MTU': 'Mitsubishi',\n",
      "    'SNE': 'Sony',\n",
      "    'F': 'Ford',\n",
      "    'HMC': 'Honda',\n",
      "    'NAV': 'Navistar',\n",
      "    'NOC': 'Northrop Grumman',\n",
      "    'BA': 'Boeing',\n",
      "    'KO': 'Coca Cola',\n",
      "    'MMM': '3M',\n",
      "    'MCD': 'Mc Donalds',\n",
      "    'PEP': 'Pepsi',\n",
      "    'KFT': 'Kraft Foods',\n",
      "    'K': 'Kellogg',\n",
      "    'UN': 'Unilever',\n",
      "    'MAR': 'Marriott',\n",
      "    'PG': 'Procter Gamble',\n",
      "    'CL': 'Colgate-Palmolive',\n",
      "    'NWS': 'News Corp',\n",
      "    'GE': 'General Electrics',\n",
      "    'WFC': 'Wells Fargo',\n",
      "    'JPM': 'JPMorgan Chase',\n",
      "    'AIG': 'AIG',\n",
      "    'AXP': 'American express',\n",
      "    'BAC': 'Bank of America',\n",
      "    'GS': 'Goldman Sachs',\n",
      "    'AAPL': 'Apple',\n",
      "    'SAP': 'SAP',\n",
      "    'CSCO': 'Cisco',\n",
      "    'TXN': 'Texas instruments',\n",
      "    'XRX': 'Xerox',\n",
      "    'LMT': 'Lookheed Martin',\n",
      "    'WMT': 'Wal-Mart',\n",
      "    'WAG': 'Walgreen',\n",
      "    'HD': 'Home Depot',\n",
      "    'GSK': 'GlaxoSmithKline',\n",
      "    'PFE': 'Pfizer',\n",
      "    'SNY': 'Sanofi-Aventis',\n",
      "    'NVS': 'Novartis',\n",
      "    'KMB': 'Kimberly-Clark',\n",
      "    'R': 'Ryder',\n",
      "    'GD': 'General Dynamics',\n",
      "    'RTN': 'Raytheon',\n",
      "    'CVS': 'CVS',\n",
      "    'CAT': 'Caterpillar',\n",
      "    'DD': 'DuPont de Nemours'}\n",
      "\n",
      "tickers = [k for k,v in symbol_dict.items()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stocks = DataReader(tickers, \"yahoo\", start=start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stocks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We now have a DataFrame that is ordered by both date and the ticker\n",
      "* This is commonly called Panel data (hence the Panel class)\n",
      "* We want to get the percentage difference in closing prices"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "returns = np.log(stocks[\"Adj Close\"]).diff() * 100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "returns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We have one missing value that we need to drop"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "returns[[\"AAPL\",\"AIG\"]].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "returns = returns.dropna()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Demean the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import zscore"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = zscore(returns.values, axis=0) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Let's use scikit-learn for this"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import RandomState\n",
      "from sklearn.decomposition import FactorAnalysis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Say we want to know the five underlying factors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA().fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(np.cumsum(pca.explained_variance_ratio_))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot(pca.explained_variance_ratio_[:10])\n",
      "ax.set_ylabel(\"Explained Variance Ratio\")\n",
      "ax.set_xlabel(\"Components\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This **Scree plot** suggests maybe we should pick ~2-3 components\n",
      "* You usually cut-off the factors after the \"elbow\" in the plot\n",
      "* I'm going to pick 5 for illustration purposes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_components = 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stock_model = FactorAnalysis(n_components=n_components)\n",
      "stock_model.fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* (n_components, n_features) array of loadings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stock_model.components_.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_loadings = pandas.DataFrame(stock_model.components_.T, index=returns.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_loadings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_data = stock_model.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "new_df = pandas.DataFrame((new_data[:,:2]), index=returns.index)\n",
      "new_df.plot(ax=ax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Unemployment Factors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%loadpy https://gist.github.com/jseabold/3912533/raw/d958b515f602f6e73f7b16d8bc412bc8d1f433d9/state_abbrevs.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states_abbrev_dict = {\n",
      "        'AK': 'Alaska',\n",
      "        'AL': 'Alabama',\n",
      "        'AR': 'Arkansas',\n",
      "        'AZ': 'Arizona',\n",
      "        'CA': 'California',\n",
      "        'CO': 'Colorado',\n",
      "        'CT': 'Connecticut',\n",
      "        'DC': 'District of Columbia',\n",
      "        'DE': 'Delaware',\n",
      "        'FL': 'Florida',\n",
      "        'GA': 'Georgia',\n",
      "        'HI': 'Hawaii',\n",
      "        'IA': 'Iowa',\n",
      "        'ID': 'Idaho',\n",
      "        'IL': 'Illinois',\n",
      "        'IN': 'Indiana',\n",
      "        'KS': 'Kansas',\n",
      "        'KY': 'Kentucky',\n",
      "        'LA': 'Louisiana',\n",
      "        'MA': 'Massachusetts',\n",
      "        'MD': 'Maryland',\n",
      "        'ME': 'Maine',\n",
      "        'MI': 'Michigan',\n",
      "        'MN': 'Minnesota',\n",
      "        'MO': 'Missouri',\n",
      "        'MS': 'Mississippi',\n",
      "        'MT': 'Montana',\n",
      "        'NC': 'North Carolina',\n",
      "        'ND': 'North Dakota',\n",
      "        'NE': 'Nebraska',\n",
      "        'NH': 'New Hampshire',\n",
      "        'NJ': 'New Jersey',\n",
      "        'NM': 'New Mexico',\n",
      "        'NV': 'Nevada',\n",
      "        'NY': 'New York',\n",
      "        'OH': 'Ohio',\n",
      "        'OK': 'Oklahoma',\n",
      "        'OR': 'Oregon',\n",
      "        'PA': 'Pennsylvania',\n",
      "        'RI': 'Rhode Island',\n",
      "        'SC': 'South Carolina',\n",
      "        'SD': 'South Dakota',\n",
      "        'TN': 'Tennessee',\n",
      "        'TX': 'Texas',\n",
      "        'UT': 'Utah',\n",
      "        'VA': 'Virginia',\n",
      "        'VT': 'Vermont',\n",
      "        'WA': 'Washington',\n",
      "        'WI': 'Wisconsin',\n",
      "        'WV': 'West Virginia',\n",
      "        'WY': 'Wyoming'\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "merge = lambda x, y : x.merge(y, left_index=True, right_index=True)\n",
      "\n",
      "dta = reduce(merge, [DataReader(name+\"UR\", \"fred\", start=\"1976-1-1\") for name in states_abbrev_dict])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dta[[\"GAUR\",\"DCUR\",\"TNUR\"]].tail(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = (dta - dta.mean()).div(dta.std(ddof=0), axis=1).values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA().fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot(pca.explained_variance_ratio_[:10])\n",
      "ax.set_ylabel(\"Explained Variance Ratio\")\n",
      "ax.set_xlabel(\"Components\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unemp_model = FactorAnalysis(n_components=5).fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_loadings = pandas.DataFrame(unemp_model.components_.T, index=dta.columns).sort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_loadings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y1,Y2 = unemp_model.transform(X)[:,:2].T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unemp = DataReader(\"UNRATE\", \"fred\", start=\"1976-1-1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unemp[\"comp1\"] = -Y1\n",
      "unemp[\"comp2\"] = -Y2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(unemp.corr())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unemp.plot(figsize=(6,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}