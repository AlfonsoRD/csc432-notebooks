{
 "metadata": {
  "name": "week14-01-dimension_reduction"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Dimensionality Reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Dimensionality reduction methods, unsurprisingly, try to reduce the number of dimensions in a dataset\n",
      "* Why?\n",
      "  * Curse of dimensionality\n",
      "  * Computational cost\n",
      "  * Reduces noise \n",
      "* Three different ways to accomplish dimensionality reduction\n",
      "  * feature selection (e.g., seeing which features are correlated with output)\n",
      "  * feature derivation (e.g., applying transforms to change the coordinate system)\n",
      "  * clustering"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Curse of Dimensionality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The curse of dimensionality has a few different meanings\n",
      "* In machine learning, we often want to separate data into its different dimensions\n",
      "* As the input space increases you will get fewer and fewer examples in any part of the sample space\n",
      "* Practically, this means that the more features you have, the more samples (observations) you will need and this increases non-linearly"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linear Discriminant Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Linear Discriminant Analysis** is a method of dimensionality reduction credited to Fisher (1936)\n",
      "* The idea is to reduce the dimensionality of the data while preserving as much of the class discriminatory information as possible\n",
      "* Consider the following two groups"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(123)\n",
      "X = np.random.multivariate_normal([0,0], [[.1,0],[0,.1]], size=19)\n",
      "X[:8] -= [1, .25]\n",
      "X[8:] += [1, .25]\n",
      "labels = np.array([0]*8 + [1]*11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Projection Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Let's look at two **projections** of the data given above\n",
      "* Recall that any line can be written as a vector $\\boldsymbol{w}$\n",
      "* The projection of one point $\\boldsymbol{x}$ onto a vector $\\boldsymbol{w}$ can be written $z=\\boldsymbol{w}^\\prime \\boldsymbol{x}$\n",
      "* This is the scalar that is the distance along the vector $\\boldsymbol{w}$ that we need to go to find the projection point"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# arbitrary direction on the line y = 1/2*x\n",
      "w = np.array([[2.], [1.]])\n",
      "print w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.array([[1.],[2]])\n",
      "print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(w.T, x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# example worked in class\n",
      "\n",
      "y = lambda x : 1/2.*x\n",
      "xx = np.linspace(-1, 4)\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot(xx, y(xx))\n",
      "ax.plot(x[0], x[1], 'ko')\n",
      "ax.plot([0,x[0]],[0,x[1]], 'k--')\n",
      "\n",
      "ax.hlines(0, -1, 4, 'k')\n",
      "ax.vlines(0, -1, 4, 'k')\n",
      "ax.set_xlim(-1, 4)\n",
      "ax.set_ylim(-1, 4);\n",
      "ax.grid(False);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(w.T, x)/np.dot(w.T, w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point = np.dot(w.T, x)/np.dot(w.T, w) * w\n",
      "print point"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.plot(xx, y(xx))\n",
      "ax.plot(x[0], x[1], 'ko')\n",
      "ax.plot([0,x[0]],[0,x[1]], 'k--')\n",
      "ax.plot(point[0], point[1], 'ko')\n",
      "ax.plot([x[0], point[0]], [x[1], point[1]], 'k--')\n",
      "\n",
      "ax.hlines(0, -1, 4, 'k')\n",
      "ax.vlines(0, -1, 4, 'k')\n",
      "ax.set_xlim(-1, 4)\n",
      "ax.set_ylim(-1, 4);\n",
      "ax.grid(False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The following picture shows what happens if we project our dataset onto some vector $\\boldsymbol{w}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "#x = np.linspace(-.25, .25, 50)\n",
      "y = np.linspace(-1.5, 1.5)\n",
      "x = y/-7\n",
      "ax.plot(x, y)\n",
      "ax.set_ylim(-1.5, 1.5)\n",
      "\n",
      "from matplotlib.collections import LineCollection\n",
      "\n",
      "# direction vector for line \n",
      "S = np.array([x[5], y[5]])\n",
      "\n",
      "# orthogonally project all of X1 onto the line\n",
      "X1_proj = (np.dot(X[:8,None],S)/np.dot(S,S)*S)\n",
      "ax.scatter(*X1_proj.T, color='k')\n",
      "\n",
      "lc = LineCollection(zip(X[:8],X1_proj))\n",
      "ax.add_collection(lc)\n",
      "\n",
      "X2_proj = (np.dot(X[8:,None],S)/np.dot(S,S)*S)\n",
      "ax.scatter(*X2_proj.T, color='k')\n",
      "lc2 = LineCollection(zip(X[8:],X2_proj), color='red')\n",
      "ax.add_collection(lc2)\n",
      "\n",
      "\n",
      "ax.annotate(\"w\", (x[-10], y[-10]), (5,0), \n",
      "            textcoords=\"offset points\", fontsize=16)\n",
      "ax.set_title(\"Not linearly separable\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "x = np.linspace(-2, 2, 50)\n",
      "#y = np.linspace(-1.25, -.75, 50)\n",
      "#x = y/.2\n",
      "y = .1*x-1\n",
      "ax.plot(x, y)\n",
      "ax.set_ylim(-1.5, 1.5)\n",
      "ax.set_xlim(-2, 2)\n",
      "\n",
      "#from matplotlib.collections import LineCollection\n",
      "\n",
      "# direction vector for line from origin\n",
      "S = np.array([x[20], y[20]+1])\n",
      "\n",
      "# orthogonally project all of X1 onto the line\n",
      "X1_proj = (np.dot(X[:8,None],S)/np.dot(S,S)*S)\n",
      "# but this line is through the origin, so reflect points\n",
      "X1_proj[:,1] -= 1\n",
      "X1_proj[:,0] += .1\n",
      "ax.scatter(*X1_proj.T, color='k')\n",
      "\n",
      "lc = LineCollection(zip(X[:8],X1_proj))\n",
      "ax.add_collection(lc)\n",
      "\n",
      "X2_proj = (np.dot(X[8:,None],S)/np.dot(S,S)*S)\n",
      "X2_proj[:,1] -= 1\n",
      "X2_proj[:,0] += .1\n",
      "ax.scatter(*X2_proj.T, color='k')\n",
      "lc2 = LineCollection(zip(X[8:],X2_proj), color='red')\n",
      "ax.add_collection(lc2)\n",
      "\n",
      "ax.annotate(\"w\", (x[25],y[25]), (0,5), textcoords=\"offset points\",\n",
      "            fontsize=16)\n",
      "ax.grid(False);\n",
      "ax.set_title(\"Linearly separable\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In order to find a good projection, however, we need to define a measure of separation\n",
      "* First we will compute some statistics of the data\n",
      "* Maybe we could maximize the distance between the projected the means?\n",
      "* This doesn't account for the standard deviation within classes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Fisher's LDA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Fisher proposed maximizing the difference between means, normalized by a measure of within-class scatter\n",
      "* To see how this solution works we need to define some statistics on our dataset\n",
      "* Let's start with means of each class, $\\mu_1$ and $\\mu_2$, the overall mean $\\mu$, and the covariance of each class with itself $\\sum_j(x_j-\\mu)(x_j-\\mu)^{\\prime}$\n",
      "* The principal insight of LDA is what the covariance matrix can tell us about the **scatter** within a dataset\n",
      "  * **scatter** is found by multiplying the covariance by $p_c$ the probability of a point belonging to that class (the number of datapoints in that class / total samples)\n",
      "* Adding the values of this for all the classes gives us a measure of the **within-class scatter**\n",
      "\n",
      "$$S_W=\\sum_c\\sum_{j\\in c}p_c(\\boldsymbol{x}_j-\\boldsymbol{\\mu}_c)(\\boldsymbol{x}_j-\\boldsymbol{\\mu}_c)^{\\prime}$$\n",
      "\n",
      "* If the dataset is easy to separate into classes, then this within-class scatter should be small\n",
      "  * Each class is clustered tightly together\n",
      "* We also want the distance *between* classes to be large, the **between-classes scatter**\n",
      "\n",
      "$$S_B=\\sum_c(\\boldsymbol{u}_c-\\boldsymbol{\\mu})(\\boldsymbol{u}_c-\\boldsymbol{\\mu})^{\\prime}$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "uc_1 = np.mean(X[:8], 0)\n",
      "uc_2 = np.mean(X[8:], 0)\n",
      "center_x = np.r_[uc_1[0], uc_2[0]]\n",
      "center_y = np.r_[uc_1[1], uc_2[1]]\n",
      "\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "ax.plot(uc_1[0], uc_1[1], \"kx\", uc_2[0], uc_2[1], \"kx\", markersize=10)\n",
      "ax.plot(center_x, center_y, \"k\")\n",
      "ax.annotate(\"$S_B$\", (center_x.mean(), center_y.mean()), xytext=(-10, 15),\n",
      "            textcoords=\"offset points\", fontsize=20)\n",
      "\n",
      "slope = np.diff(center_y)/np.diff(center_x)\n",
      "\n",
      "sw1 = 8./19.*np.sum((X[:8] - uc_1)**2)\n",
      "theta = np.arctan(slope)\n",
      "y12 = sw1/2 * np.sin(theta)\n",
      "y11 = -y12\n",
      "x12 = sw1/2 * np.cos(theta)\n",
      "x11 = -x12\n",
      "sw1_x = np.r_[x11,x12]+uc_1[0]\n",
      "# move it below the cluster\n",
      "sw1_y = np.r_[y11-(y12-X[:8,1].min()),+X[:8,1].min()]\n",
      "ax.plot(sw1_x, sw1_y, 'k')\n",
      "ax.annotate(\"$S_{W1}$\", (np.mean(sw1_x),np.mean(sw1_y)),\n",
      "            (-5,-15), textcoords=\"offset points\", \n",
      "            fontsize=16)\n",
      "\n",
      "sw2 = 11./19.*np.sum((X[8:] - uc_2)**2)\n",
      "y22 = sw2/2 * np.sin(theta)\n",
      "y21 = -y22\n",
      "x22 = sw2/2 * np.cos(theta)\n",
      "x21 = -x22\n",
      "sw2_x = np.r_[x21,x22]+uc_2[0]\n",
      "# move it below the cluster\n",
      "sw2_y = np.r_[y21-(y22-X[8:,1].min()),+X[8:,1].min()]\n",
      "ax.plot(sw2_x, sw2_y, 'k')\n",
      "ax.annotate(\"$S_{W2}$\", (np.mean(sw2_x),np.mean(sw2_y)),\n",
      "            (-5,-15), textcoords=\"offset points\", \n",
      "            fontsize=16);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Datasets that are easy to separate into different classes (classes that are **discriminable**) should have $S_B/S_W$ as large as possible.\n",
      "* The between cluster variance should be much larger than the weighted within-cluster variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* What does this tell us about data reduction?\n",
      "* Well we might want to follow the rule that $S_B/S_W$ should be large when reducing the number of dimensions of our data\n",
      "* We can compute a projection of our data onto a vector $\\boldsymbol{w}$ as the linear function $y=w^Tx$\n",
      "* This is Fisher's Linear Discriminant function"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* The mean can be treated like a datapoint such that $\\boldsymbol{\\mu}_c^\\prime=w^T\\cdot\\boldsymbol{\\mu}_c$\n",
      "* The within-class scatter can be computed by replacing $x_j$ with its projection too\n",
      "\n",
      "$$\\sum_c \\sum_{j\\in c}p_c(\\boldsymbol{w}^T\\cdot(\\boldsymbol{x}_j-\\boldsymbol{\\mu}_c))(\\boldsymbol{w}^T\\cdot(\\boldsymbol{x}_j-\\boldsymbol{u}_c))^T=\\boldsymbol{w}^TS_W\\boldsymbol{w}$$\n",
      "\n",
      "* The between-class scatter is\n",
      "\n",
      "$$\\sum_c \\boldsymbol{w}^T(\\boldsymbol{\\mu}_c-\\boldsymbol{\\mu})(\\boldsymbol{\\mu_c}-\\boldsymbol{\\mu})^T\\boldsymbol{w}=\\boldsymbol{w}^TS_B\\boldsymbol{w}$$\n",
      "\n",
      "* Our criterion for discriminability is now \n",
      "\n",
      "$$\\frac{\\boldsymbol{w}^TS_w\\boldsymbol{w}}{\\boldsymbol{w^T}S_B\\boldsymbol{w}}$$\n",
      "\n",
      "* We want to choose the $\\boldsymbol{w}$ that maximizes the separation\n",
      "* To maximize this, we differentiate it and set the derivative equal to 0\n",
      "\n",
      "$$\\frac{S_B\\boldsymbol{w}(\\boldsymbol{w}^TS_w\\boldsymbol{w})-S_W\\boldsymbol{w}(\\boldsymbol{w}^TS_B\\boldsymbol{w})}{(\\boldsymbol{w^T}S_B\\boldsymbol{w})^2}=0$$\n",
      "\n",
      "* We can rearrange this to give\n",
      "\n",
      "$$S_W\\boldsymbol{w}=\\frac{\\boldsymbol{w}^TS_w\\boldsymbol{w}}{\\boldsymbol{w}^TS_B\\boldsymbol{w}}S_B\\boldsymbol{w}$$\n",
      "\n",
      "* Solving for $\\boldsymbol{w}$ requires computing the **generalized eigenvectors** of $S_w^{-1}S_B$\n",
      "* For the two-class case $\\boldsymbol{w}$ is in the direction of $S_W^{-1}(\\mu_1-\\mu_2)$ since the ratio is a scalar it doesn't effect the direction and we can ignore it"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "LDA with Scikit-Learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.lda import LDA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = LDA(n_components=1).fit(X, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can recover our reduced dimension data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# direction vector for line from origin\n",
      "S = lda.scalings_\n",
      "print S"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(S.T,S)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(X[:8], S)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*X.T, color=[\"blue\"]*8+[\"red\"]*11)\n",
      "ax.set_ylim(-1.5, 1.5)\n",
      "ax.set_xlim(-2, 2)\n",
      "\n",
      "from matplotlib.collections import LineCollection\n",
      "\n",
      "# orthogonally project all of X1 onto the line\n",
      "X1_proj = np.dot(X[:8],S)/np.dot(S.T,S)*S.T\n",
      "ax.scatter(*X1_proj.T, color='k')\n",
      "\n",
      "lc = LineCollection(zip(X[:8],X1_proj))\n",
      "ax.add_collection(lc)\n",
      "\n",
      "X2_proj = np.dot(X[8:],S)/np.dot(S.T,S)*S.T\n",
      "ax.scatter(*X2_proj.T, color='k')\n",
      "lc2 = LineCollection(zip(X[8:],X2_proj), color='red')\n",
      "ax.add_collection(lc2)\n",
      "\n",
      "ax.grid(False);\n",
      "ax.set_title(\"LDA Solution\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Fisher developed LDA to deal with the Iris dataset\n",
      "* So let's try that one\n",
      "* What do you get using the Iris data?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "data = iris.data\n",
      "target = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Things to be aware of\n",
      "* LDA is actually maximizing a log-likelihood *assuming* the data is normally distributed and homegenous\n",
      "* It is very similar in practice to multinomial logistic regression "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Principal Components Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* But what if we don't have labelled data?\n",
      "* Or what if we don't want to use the labels?\n",
      "* We can still perform data reduction without using targets\n",
      "* The idea is that we can find a new set of coordinate axes that might make it clear that some dimensions of the data are not needed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1234)\n",
      "X = np.random.multivariate_normal([1,1], [[.05,.1],[.1,.25]], size=40)\n",
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "ax.scatter(*X.T)\n",
      "ax.hlines(0, -.5, 2.5)\n",
      "ax.vlines(0, -.5, 2.5)\n",
      "ax.set_xlim(-.5, 2.5)\n",
      "ax.set_ylim(-.5, 2.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = X - X.mean(0)\n",
      "cov = np.dot(Y.T, Y)\n",
      "eigvals, eigvecs = np.linalg.eig(cov)\n",
      "\n",
      "eigvecs *= eigvals**.5\n",
      "dir1, dir2 = eigvecs.T + X.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print eigvals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "#ax.scatter(*pca.transform(X).T)\n",
      "ax.scatter(*X.T)\n",
      "ax.plot([X_mean[0],dir1[0]], [X_mean[1],dir1[1]], 'k', lw=1.5)\n",
      "ax.plot([X_mean[0],dir2[0]], [X_mean[1],dir2[1]], 'k', lw=1.5)\n",
      "ax.set_xlim(-.5, 2.5)\n",
      "ax.set_ylim(-.5, 2.5);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The lines can be considered a rotated axes for this dataset\n",
      "* The lengths signify the importance of each dimension\n",
      "* You can see that the \"y\" axis might not be that important and it might be possible to ignore it\n",
      "* But how do we choose the \"important\" axes?\n",
      "* If you peek above, you may get an idea"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* One method is called **Principal Component Analysis**\n",
      "* The basic idea of a **principal component** is that it is a direction in the data with the largest variation\n",
      "* The PCA algorithm centers the data by subtracint the mean, then chooses the direction with largest variation\n",
      "* It places an axis in that direction, then looks at the variation that remains\n",
      "* It finds another axis that is orthogonal to the first and covers as much the remaining variation as possible\n",
      "* This continues until there are no more axes\n",
      "* The end result is a dataset with a diagonal covariance matrix. Why?\n",
      "* The axes found last likely have very little variation and can be removed without affected the variability in the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Formally, say we have a dataset $\\boldsymbol{X}$\n",
      "* We want to rotate it so that the data lies along the directions of maximum variation\n",
      "* To do so multiply $\\boldsymbol{X}$ by a rotation matrix $\\boldsymbol{P}^T$ such that\n",
      "\n",
      "$$\\boldsymbol{Y}=\\boldsymbol{P}^T\\boldsymbol{X}$$\n",
      "\n",
      "where $\\boldsymbol{P}$ is chosen so that the covariance matrix of $\\boldsymbol{Y}$ is diagonal\n",
      "\n",
      "$$\\operatorname{cov}(\\boldsymbol{Y})=\\operatorname{cov}(\\boldsymbol{P}^T\\boldsymbol{X})=\\left(\\begin{array}{ccccc} \\cr\n",
      "\\lambda_1 & 0 & 0 & \\dots & 0 \\cr\n",
      "0 & \\lambda_2 & 0 & \\dots & 0 \\cr\n",
      "\\vdots & \\ddots & \\dots & \\dots & \\dots \\cr\n",
      "0 & 0 & 0 & \\dots & \\lambda_N \\cr\n",
      "\\end{array}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Aside**: rotation matrices\n",
      "* Rotation matrices are square matrices, with real entries\n",
      "* They are orthogonal and their determinant = 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta = np.deg2rad(45) # 45 degrees in radians\n",
      "\n",
      "# anti-clockwise rotation about the origin\n",
      "P = np.array([[np.cos(theta), -np.sin(theta)],\n",
      "              [np.sin(theta), np.cos(theta)]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point = np.array([[2],\n",
      "                  [1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*point, s=32)\n",
      "ax.plot([0, point[0]], [0, point[1]], 'k--')\n",
      "ax.hlines(0, -1, 3)\n",
      "ax.vlines(0, -1, 3)\n",
      "ax.set_xlim(-1, 3)\n",
      "ax.set_ylim(-1, 3);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can rotate this point by $\\boldsymbol{P}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "point2 = np.dot(P, point)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(2, 1, s=32)\n",
      "ax.plot([0, point[0]], [0, point[1]], 'k--')\n",
      "ax.scatter(*point2.T[0], s=32)\n",
      "ax.plot([0, point2[0]], [0, point2[1]], 'k--')\n",
      "ax.hlines(0, -1, 3)\n",
      "ax.vlines(0, -1, 3)\n",
      "ax.set_xlim(-1, 3)\n",
      "ax.set_ylim(-1, 3);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Recall the definition of covariance\n",
      "\n",
      "$$\\operatorname{cov}(Y)=\\mathbb{E}[YY^T]$$\n",
      "\n",
      "* Substitute in our rotated dataset for $Y$\n",
      "\n",
      "$$\\begin{aligned}\\operatorname{cov}(\\boldsymbol{Y})&=\\mathbb{E}[\\boldsymbol{Y}\\boldsymbol{Y}^T] \\cr\n",
      "&=\\mathbb{E}[(\\boldsymbol{P}^T\\boldsymbol{X})(\\boldsymbol{P}^T\\boldsymbol{X})^T] \\cr\n",
      "&=\\mathbb{E}[(\\boldsymbol{P}^T\\boldsymbol{X})(\\boldsymbol{X}^T\\boldsymbol{P})] \\cr\n",
      "&=\\boldsymbol{P}^TE(\\boldsymbol{X}\\boldsymbol{X}^T)\\boldsymbol{P} \\cr\n",
      "&=\\boldsymbol{P}^T\\operatorname{cov}(\\boldsymbol{X})\\boldsymbol{P}\n",
      "\\end{aligned}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This means that\n",
      "\n",
      "$$\\boldsymbol{P}\\operatorname{cov}(\\boldsymbol{Y})=\\boldsymbol{P}\\boldsymbol{P}^T\\operatorname{cov}(\\boldsymbol{X})\\boldsymbol{P}=\\operatorname{cov}(\\boldsymbol{X})\\boldsymbol{P}$$\n",
      "\n",
      "* The second equality follows from the fact that $P^T=P^{-1}$ for rotation matrices\n",
      "* Since the covariance is diagonal we can write $\\boldsymbol{P}$ as a set of column vectors\n",
      "\n",
      "$$\\boldsymbol{P}\\operatorname{cov}(\\boldsymbol{Y})=[\\lambda_1\\boldsymbol{p}_1, \\lambda_2\\boldsymbol{p}_2, \\dots, \\lambda_N\\boldsymbol{p}_N]$$\n",
      "\n",
      "* Let $\\boldsymbol{Z}=\\operatorname{cov}(X)$\n",
      "* And $\\boldsymbol{\\lambda}=\\operatorname{diag}([\\lambda_1, \\lambda_2, \\dots, \\lambda_N])$\n",
      "* We can now write\n",
      "\n",
      "$$\\boldsymbol{P}\\boldsymbol{\\lambda}=\\boldsymbol{Z}\\boldsymbol{P}$$\n",
      "\n",
      "or\n",
      "\n",
      "$$\\boldsymbol{\\lambda}=\\boldsymbol{P}^{-1}\\boldsymbol{Z}\\boldsymbol{P}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $\\boldsymbol{\\lambda}$ is a diagonal matrix, so all it does is rescale the rotation matrix $\\boldsymbol{P}$\n",
      "* This means we have found a matrix $P$ such that "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* $\\boldsymbol{\\lambda}$ are called the eigenvalues of $\\boldsymbol{Z}$\n",
      "* The eigenvalues of a matrix $\\boldsymbol{Z}$ are the roots to the *characteristic equation*, or *characteristic polynomial*\n",
      "\n",
      "    $$determinant(\\boldsymbol{Z}-\\boldsymbol{\\lambda}I)=|\\boldsymbol{Z}-\\boldsymbol{\\lambda}I|=0$$\n",
      "    </br>\n",
      "* Let $\\lambda_1$ be an eigenvalue of $Z$, then there is a vector $\\boldsymbol{P_1}$ such that\n",
      "\n",
      "$$\\lambda\\boldsymbol{P_1}=\\boldsymbol{Z}\\boldsymbol{P_1}$$\n",
      "\n",
      "* The vector $\\boldsymbol{P_1}$ is called the eigenvector of $\\boldsymbol{Z}$ associated with the eigenvalue $\\lambda_1$\n",
      "* Notice that $\\boldsymbol{P_1}$ is not unique. It is only a direction vector and can have arbitrary scaling.\n",
      "* It is common to scale all of the eigenvectors of a matrix to have unit length"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "An Example using NumPy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z = np.array([[ 3., 0., 1.],\n",
      "              [4., 6., 2.],\n",
      "              [3., 1., 2.]]) \n",
      "\n",
      "eigvals, eigvecs = np.linalg.eig(Z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print eigvals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print eigvecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Eigenvectors are in the columns\n",
      "* Unit length"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(eigvecs**2, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(eigvecs, np.diag(eigvals))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(Z, eigvecs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first eigenvalue"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(eigvecs[:,0], np.dot(Z, eigvecs[:,0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "PCA Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Takes some data $\\boldsymbol{X}$\n",
      "* Subtract the mean\n",
      "* Get the covariance matrix $\\operatorname{cov}(\\boldsymbol{X})=\\frac{1}{N}\\boldsymbol{X}^T\\boldsymbol{X}$\n",
      "* Calculate the eigenvalues and eigenvectors\n",
      "* Sort the eigenvectors in *decreasing* order of eigenvalues\n",
      "* Ignore eigenvectors with an associated eigenvalue less than some $\\epsilon$, leaving $L$ dimensions in the data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "An Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(12345)\n",
      "\n",
      "cov = [[.4,2],[2,2]]\n",
      "\n",
      "rvs = np.random.multivariate_normal([6,-1], cov, size=500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(6,6))\n",
      "\n",
      "ax.scatter(*rvs.T, alpha=.6)\n",
      "#ax.set_xlim(3, 9)\n",
      "#ax.set_ylim(-4, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Using NumPy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rvs_demean = rvs - rvs.mean(0)\n",
      "cov = 1./len(rvs)*np.dot(rvs_demean.T, rvs_demean)\n",
      "print cov"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.cov(rvs_demean, rowvar=0, ddof=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvals, eigvecs = np.linalg.eig(cov)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print eigvals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idx = np.argsort(eigvals)[::-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvecs[:,idx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Take only the largest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigvecs = eigvecs[:,idx][:,0,None]\n",
      "print eigvecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Project the data onto the eigenvectors and translate back to mean"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.dot(rvs_demean, eigvecs)\n",
      "y = np.dot(x, eigvecs.T) + rvs.mean(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, axes = plt.subplots(1, 2, figsize=(12,6), sharex=True, sharey=True)\n",
      "\n",
      "axes[0].scatter(*rvs.T, alpha=.6)\n",
      "axes[1].scatter(*y.T, alpha=.6)\n",
      "#ax.set_xlim(3, 9)\n",
      "#ax.set_ylim(-4, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Using scikit-learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(rvs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.n_components = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = pca.fit_transform(rvs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reduced dimension"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reduced dimension projected in direction of first principal component"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_direc = np.dot(y, pca.components_) + pca.mean_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, axes = plt.subplots(1, 2, figsize=(12,6), sharex=True, sharey=True)\n",
      "\n",
      "axes[0].scatter(*rvs.T, alpha=.6)\n",
      "axes[1].scatter(*y_direc.T, alpha=.6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Iris dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Do the same for the Iris dataset\n",
      "* Reduce the number of dimensions to 2 using PCA\n",
      "* Plot the data in two dimensions, using colors to differentiate between classes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Kernel PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Kernel Trick\n",
      "* Kernel PCA Algorithm"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Factor Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In factor analysis, the researcher asks whether the data that is observed can be explained by a smaller number of unobserved, uncorrelated **factors**, or **latent variables**\n",
      "* The goal of factor analysis is to uncover the independent factors and the noise inherent in their measurement\n",
      "* Factor analysis is used often in psychology, social sciences, and finance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Suppose we have a dataset $\\boldsymbol{X}$ that is $N\\times M$ and has a covariance matrix $\\boldsymbol{\\Sigma}$\n",
      "* Assume that $\\boldsymbol{X}$ is cenetered such that each column has mean zero\n",
      "* Ie., for each column $j$ we have $\\boldsymbol{b}_j=\\boldsymbol{x}_j-\\boldsymbol{\\mu}_j$\n",
      "* We can write the assumed model as\n",
      "\n",
      "$$\\boldsymbol{X}=\\boldsymbol{W}\\boldsymbol{b}+\\boldsymbol{\\epsilon}$$\n",
      "\n",
      "where \n",
      "\n",
      "* $\\boldsymbol{X}$ is the data and $\\boldsymbol{\\epsilon}$ is the noise\n",
      "* $\\boldsymbol{b}$ are the latent factors such that it's a matrix of column variables $\\boldsymbol{b}=[\\boldsymbol{b}_1, \\boldsymbol{b}_2, \\dots, \\boldsymbol{b}_k]$\n",
      "* and $\\boldsymbol{W}$ are called the **factor loadings**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We can now rewrite the covariance matrix of $\\boldsymbol{X}$ as\n",
      "\n",
      "$$\\begin{aligned}\\operatorname{cov}(X)&=\\operatorname{cov}(\\boldsymbol{W}\\boldsymbol{b}+\\boldsymbol{\\epsilon}) \\cr\n",
      "&=\\operatorname{cov}(\\boldsymbol{W}\\boldsymbol{b})+\\operatorname{cov}(\\boldsymbol{\\epsilon}) \\cr\n",
      "&=\\boldsymbol{W}\\boldsymbol{W}^T+\\Psi\n",
      "\\end{aligned}$$\n",
      "\n",
      "* The third line follows from the fact that $\\operatorname{cov}(\\boldsymbol{b})=\\boldsymbol{I}$ because the factors are uncorrelated\n",
      "* Assuming the data is demeaned, then\n",
      "\n",
      "$$X \\sim \\mathcal{N}(0, WW^T + \\Psi)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* For Factor Analysis, we need one more assumption on the structure of the error covariance $\\boldsymbol{\\Psi}$\n",
      "* We will assume that $\\boldsymbol{\\Psi}=\\operatorname{diag}({\\psi_1, \\psi_2, \\dots, \\psi_m})$\n",
      "* A related model, called Probabilistic PCA, assumes that $\\boldsymbol{\\Psi}=\\sigma^2 \\boldsymbol{I})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Since we have a distribution, to estimate both $\\boldsymbol{W}$ and $\\boldsymbol{\\Psi}$ we would write down the likelihood of the model\n",
      "* We can maximize this likelihood by using a technique called Expectations Maximization or the EM algorithm\n",
      "* We won't discuss this algorithm for now"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import RandomState\n",
      "from sklearn.decomposition import FactorAnalysis\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "\n",
      "n_row, n_col = 2, 3\n",
      "n_components = n_row * n_col\n",
      "image_shape = (64, 64)\n",
      "rng = RandomState(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try: # 2/3 bug\n",
      "    dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
      "    faces = dataset.data\n",
      "except: # on python 2.6/2.7\n",
      "    # caching\n",
      "    from os.path import exists, join\n",
      "    from sklearn.base import get_data_home\n",
      "    from sklearn.externals import joblib\n",
      "    data_home = get_data_home(data_home=None)\n",
      "    TARGET_FILENAME = \"olivetti.pkz\"\n",
      "    if not exists(join(data_home, TARGET_FILENAME)):\n",
      "        \n",
      "        from urllib2 import urlopen\n",
      "        from StringIO import StringIO as BytesIO\n",
      "        from scipy.io import loadmat\n",
      "        DATA_URL = \"http://cs.nyu.edu/~roweis/data/olivettifaces.mat\"\n",
      "        fhandle = urlopen(DATA_URL)\n",
      "        buf = BytesIO(fhandle.read())\n",
      "        mfile = loadmat(buf)\n",
      "        faces = mfile['faces'].T.copy()\n",
      "        joblib.dump(faces, join(data_home, TARGET_FILENAME), compress=6)\n",
      "    else:\n",
      "        faces = joblib.load(join(data_home, TARGET_FILENAME))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples, n_features = faces.shape\n",
      "\n",
      "faces_centered = faces - faces.mean(axis=0)\n",
      "\n",
      "# local centering\n",
      "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
      "\n",
      "print(\"Dataset consists of %d faces\" % n_samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
      "    fig, axes = plt.subplots(n_row, n_col, figsize=(2. * n_col, 2.26 * n_row))\n",
      "    fig.suptitle(title, size=16)\n",
      "    for i, comp in enumerate(images):\n",
      "        ax = axes[i // n_col, i % n_col]\n",
      "        vmax = max(comp.max(), -comp.min())\n",
      "        ax.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
      "                  interpolation='nearest',\n",
      "                  vmin=-vmax, vmax=vmax)\n",
      "        ax.set_xticks(())\n",
      "        ax.set_yticks(())\n",
      "    fig.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "factor_analysis = FactorAnalysis(n_components=n_components, max_iter=2)\n",
      "\n",
      "factor_analysis.fit(faces_centered)\n",
      "\n",
      "components = factor_analysis.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_gallery(\"Factor Analysis Decomposition - %d components\" % n_components, components[:n_components])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}