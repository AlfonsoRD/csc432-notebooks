{
 "metadata": {
  "name": "week12-01-perceptron-intro"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The term **learning** refers to animal learning behaviors like remembering, adapting, and generalizing\n",
      "* **Machine learning** is applying a set of techniques to make a computer modify or adapt actions so that the actions become more accurate\n",
      "* Accuracy is measured by how well chosen actions reflect correct ones\n",
      "* **Supervised learning** - A **training set** of examples with the correct responses (**targets**) is provided. Based on this, the algorithm generalizes to respond correctly to all inputs.\n",
      "* **Unsupervised learning** - No **target set** is provided. Instead the algorithm tries to identify similaries between the inputs so that the inputs that have something in common are categorized together. In statistics, we call this **density** estimation.\n",
      "* **Reinforcement learning** - A mix of supervised and unsupervised learning. The algorithm gets told when the answer is wrong, but not how to correct it. It must try out different answers until it gets it right.\n",
      "* **Evolutionary learning** - Borrowing from evolutionary biology, a computer can learn and improve strategies based on the idea of fitness, or a score for how good the current solution is."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Supervised Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We will have a set of *training data*, which consists of input data, or **features**, and a target $(\\boldsymbol{x}_i, \\boldsymbol{t}_i)$\n",
      "* Machine learning sometimes uses **regression** to generalize (or predict) outside of the input range of the data\n",
      "* Similarly to the discrete choice models we have seen, a broad class of problems in machine learning are known as **classification problems**\n",
      "* Classification problems consist of taking input vectors and deciding which if $N$ classes they belong to based on a training set\n",
      "* There are *many* different approaches to classification\n",
      "* The unifying feature is that they all seek *decision boundaries* to separate out different classes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm, datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# import some data to play with\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data[:, :2]  # we only take the first two features.\n",
      "Y = iris.target\n",
      "\n",
      "h = .02  # step size in the mesh\n",
      "\n",
      "# we create an instance of SVM and fit out data. We do not scale our\n",
      "# data since we want to plot the support vectors\n",
      "C = 1.0  # SVM regularization parameter\n",
      "svc = svm.SVC(kernel='linear', C=C).fit(X, Y)\n",
      "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)\n",
      "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)\n",
      "lin_svc = svm.LinearSVC(C=C).fit(X, Y)\n",
      "\n",
      "# create a mesh to plot in\n",
      "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "                     np.arange(y_min, y_max, h))\n",
      "\n",
      "# title for the plots\n",
      "titles = ['SVC with linear kernel',\n",
      "          'SVC with RBF kernel',\n",
      "          'SVC with polynomial (degree 3) kernel',\n",
      "          'LinearSVC (linear kernel)']\n",
      "\n",
      "fig, axes = plt.subplots(2, 2, figsize=(8,6))\n",
      "\n",
      "for i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):\n",
      "    # Plot the decision boundary. For that, we will asign a color to each\n",
      "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    ax = axes[i / 2, i % 2]\n",
      "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "\n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    C = ax.contourf(xx, yy, Z, cmap=plt.cm.YlGnBu)\n",
      "    ax.axis('off')\n",
      "\n",
      "    # Plot also the training points\n",
      "    ax.scatter(X[:, 0], X[:, 1], c=Y)\n",
      "\n",
      "    ax.set_title(titles[i])\n",
      "fig.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Neurons"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Neurons are nerve cells that transmit information when chemical changes raise the electrical potential within the neuron\n",
      "* There is a strength between neural connections\n",
      "* How is it decided?\n",
      "* **Hebb's rule** - strength of synaptic connections is proportional to the correlation in the firing of the two connecting neurons\n",
      "* McCullock and Pitts Neurons - mathematical model of neurons from 1943\n",
      "* Modelled a neuron as\n",
      "\n",
      "1. **a set of weighted inputs** $w_i$ that correspond to the synapses, or connections between neurons\n",
      "2. **an adder** that sums the input signals\n",
      "3. **an activation function** that decides whether the neuron fires for the current input"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* On some signal $x_i$ arriving into a neuron, the decision whether or not to fire is given by\n",
      "\n",
      "$$h=\\sum_{i=1}^mw_ix_i=x^\\prime w$$\n",
      "\n",
      "which is just a weighted average of the signal\n",
      "\n",
      "* The decision to fire rests on some threshold\n",
      "* Say, if $h>\\theta$, the neuron fires. This is the *activation function*\n",
      "\n",
      "$$o=g(h)=\\begin{cases}1 \\mbox{ if }h>\\theta \\cr 0 \\mbox{ if } h\\le \\theta\\end{cases}$$\n",
      "\n",
      "* This is a binary activation function\n",
      "* Provided that the weights are chosen correctly, a network of neurons such as this can perform any computation that a computer can"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linear Discriminants"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Perceptron"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The perceptron, created in 1958, is a collection of McCullock and Pitts neurons together with a set of inputs and some weights\n",
      "* In general, we will have $m$ inputs and $n$ targets (outputs)\n",
      "* There will be one neuron for each target \n",
      "* The weights in the perceptron are written $w_{ij}$ or $\\boldsymbol{W}$ where $i$ runs over the number of inputs $m$ and $j$ runs over the number of neurons $n$\n",
      "* We want to choose the weights in such a way so that all of the neurons are fired correctly according to the targets\n",
      "* Suppose given an input vector that one of the neurons is wrong\n",
      "* There are $m$ weights that are connected to that neuron, one for each of the input nodes\n",
      "* Say the incorrect neuron is $k$, then we want to correct the weights $w_{ik}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* How do we change the weights $w_{ik}$? \n",
      "* First, we see if the weights are too big or too small\n",
      "* Compute a possible **error function** $t_k - y_k$\n",
      "* If this is positive, the neuron should have fired and didn't, so the weights need to be larger\n",
      "* Vice versa, if it's negative\n",
      "* Careful of the sign of $x_i$\n",
      "* $\\Delta w_{ik} = (t_k-y_k)\\times x_i$\n",
      "* The new weight is thus the old one plus this change\n",
      "* What if the input is 0?\n",
      "* The last thing to decide is how much to change the weight by\n",
      "* To do so we need to introduce the **learning rate** $\\eta$\n",
      "* This decides how fast the network learns\n",
      "* Thus our updating rule is\n",
      "\n",
      "$$w_{ij}\\longleftarrow w_{ij} + \\eta(t_j-y_j)\\cdot x_i$$\n",
      "\n",
      "* It should be clear by now that learning of this network will be iterative"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Learning Rate $\\eta$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The learning rate $\\eta$ could be left out by setting $\\eta=1$\n",
      "* This could lead to an *unstable* network due to the weights changing a lot\n",
      "* It is usually recommended to use a moderate learning rate $0.1 < \\eta < 0.4$\n",
      "* The cost of a smaller learning rate is that the network takes longer to learn\n",
      "* It will be more stable and more resistant to noise though"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Bias Input"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Recall the problem of a zero input\n",
      "* No matter what the weights, the neuron will never fire\n",
      "* We can't simply adjust the threshold\n",
      "* Why not?\n",
      "* All neurons would fire or not fire in the presence of zero inputs\n",
      "* The solution is to add a **bias node** with an input fixed to be -1\n",
      "* Thus, the value of the weight will change to make a neuron fire or not\n",
      "* This weight will be denoted $w_{0j}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Perceptron Learning Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Initialization**\n",
      "  - Set all of the weights $w_{ij}$ to small (positive and negative) random numbers\n",
      "* **Training**\n",
      "  * for $T$ iterations\n",
      "     * compute the activation of each neuron $j$ using the activation function $g$ <br />\n",
      "       $y_j=g\\left(\\sum_{i=0}^m w_{ij}x_i\\right)=\\begin{cases}1 \\mbox{ if } w_{ij}x_i > 0 \\cr 0 \\mbox{ if } w_{ij}x_i \\le 0 \\end{cases}$\n",
      "     * update each of the weights <br />\n",
      "       $w_{ij}\\leftarrow w_{ij}+\\eta(t_j-y_j)\\cdot x_i$\n",
      "* **Recall**\n",
      "  * compute the activation of each neuron $j$ using\n",
      "    \n",
      "    $y_j=g\\left(\\sum_{i=0}^m w_{ij}x_i\\right)=\\begin{cases}1 \\mbox{ if } w_{ij}x_i > 0 \\cr 0 \\mbox{ if } w_{ij}x_i \\le 0 \\end{cases}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The computational complexity of this algorithm is $\\operatorname{O}(Tmn)$\n",
      "* Looping over neurons and inputs is $\\operatorname{O}(mn)$\n",
      "* We perform this $T$ times"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example: Logical OR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Say we want to learn the logical OR"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array([[0., 0.],\n",
      "              [0., 1.],\n",
      "              [1., 0.],\n",
      "              [1., 1.]])\n",
      "\n",
      "target = np.array([0., 1., 1., 1.])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X\n",
      "print target[:,None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Initialize the weights $(w_0, w_1, w_2)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w = np.array([-.05, -.02, .02])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Don't forget to include the bias node"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.column_stack((-np.ones(4), X))\n",
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Feed in the first input"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.dot(X[0], w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This value is above zero, so the neuron fires, which is incorrect\n",
      "* Apply the update rule with $\\eta = .25$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eta = .25"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print w\n",
      "w = w + eta * (0 - 1)*X[0]\n",
      "print w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Now feed in the next input"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(X[1], w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Does this fire or not? Should it?\n",
      "* Update weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w = w + eta * (1 - 0)*X[1]\n",
      "print w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* How do these weights do for 3 and 4?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.dot(X[2:], w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Run it again, until you've run it $T$ times"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = X.shape[0]\n",
      "n = target.ndim == 1 and 1 or target.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_bias_node(inputs):\n",
      "    bias_node = -np.ones(len(inputs))\n",
      "    return np.column_stack((inputs, bias_node))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Adapted from the code of Stephen Marsland\n",
      "# http://www-ist.massey.ac.nz/smarsland/Code/2/pcn.py\n",
      "\n",
      "class Perceptron(object):\n",
      "    \"\"\"\n",
      "    A basic Perceptron\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    eta : float\n",
      "        The learning rate of the network. Should be between\n",
      "        0 and 1.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, eta):\n",
      "        self.eta = eta\n",
      "\n",
      "    def _initialize(self, inputs, targets, seed):\n",
      "        inputs = np.asarray(inputs)\n",
      "        # gets set to 2d in _add_bias_node\n",
      "\n",
      "        targets = np.asarray(targets)\n",
      "        if targets.ndim == 1:\n",
      "            targets = targets[:, None]\n",
      "\n",
      "        self.targets = targets\n",
      "        self.inputs = inputs\n",
      "\n",
      "        # Set up network size\n",
      "        self.m = inputs.shape[1]\n",
      "        \n",
      "        # Set up targets size\n",
      "        self.n = targets.shape[1]\n",
      "\n",
      "        # number of observations\n",
      "        self.nobs = inputs.shape[0]\n",
      "    \n",
      "        # Initialise network weights plus 1 for bias node\n",
      "        self._init_weights(seed)\n",
      "    \n",
      "    def _init_weights(self, seed):\n",
      "        if seed is not None:\n",
      "            np.random.seed(seed)\n",
      "        self.weights = np.random.rand(self.m+1, self.n)*0.1-0.05        \n",
      "    \n",
      "    def fit(self, inputs, targets, max_iter, seed=None):\n",
      "        \"\"\" \n",
      "        Trains the network.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        inputs : array-like\n",
      "            The inputs data\n",
      "        targets : array-like\n",
      "            The targets to train on\n",
      "        max_iter : int\n",
      "            The number of iterations to perform\n",
      "        seed : float, optional\n",
      "            The seed for the initialization of the weights\n",
      "        \"\"\" \n",
      "        self._initialize(inputs, targets, seed)\n",
      "        inputs = self.inputs\n",
      "        targets = self.targets\n",
      "        weights = self.weights\n",
      "        eta = self.eta\n",
      "\n",
      "        # Add the inputs that match the bias node\n",
      "        inputs = add_bias_node(inputs)\n",
      "        # Training\n",
      "        change = range(self.nobs)\n",
      "\n",
      "        for n in range(max_iter):\n",
      "            outputs = self.predict(inputs, weights, add_bias=False)\n",
      "            weights += eta * np.dot(inputs.T, targets - outputs)\n",
      "        \n",
      "            # Randomize order of inputs\n",
      "            np.random.shuffle(change)\n",
      "            inputs = inputs[change, :]\n",
      "            targets = targets[change, :]\n",
      "            \n",
      "        self.weights = weights\n",
      "\n",
      "    def predict(self, inputs=None, weights=None, add_bias=True):\n",
      "        \"\"\" Run the network forward \"\"\"\n",
      "        if weights is None:\n",
      "            try:\n",
      "                weights = self.weights\n",
      "            except:\n",
      "                raise ValueError(\"fit the classifier first \"\n",
      "                                 \"or give weights\")\n",
      "        if inputs is None:\n",
      "            inputs = self.inputs\n",
      "        if add_bias:\n",
      "            inputs = add_bias_node(inputs)\n",
      "        \n",
      "        outputs = np.dot(inputs, weights)\n",
      "\n",
      "        # Threshold the outputs\n",
      "        return np.where(outputs>0, 1, 0)\n",
      "\n",
      "\n",
      "    def confusion_matrix(self, inputs, targets, weights):\n",
      "        \"\"\"Confusion matrix\"\"\"\n",
      "        inputs = np.asarray(inputs)\n",
      "        targets = np.asarray(targets)\n",
      "\n",
      "        # Add the inputs that match the bias node\n",
      "        inputs = add_bias_node(inputs)\n",
      "        \n",
      "        outputs = np.dot(inputs, weights)\n",
      "    \n",
      "        n_classes = targets.ndim == 1 and 1 or targets.shape[1]\n",
      "\n",
      "        if n_classes == 1:\n",
      "            n_classes = 2\n",
      "            outputs = np.where(outputs > 0, 1, 0)\n",
      "        else:\n",
      "            # 1-of-N encoding\n",
      "            outputs = np.argmax(outputs, 1)\n",
      "            targets = np.argmax(targets, 1)\n",
      "\n",
      "        outputs = np.squeeze(outputs)\n",
      "        targets = np.squeeze(targets)\n",
      "\n",
      "        cm = np.histogram2d(targets, outputs, bins=n_classes)[0]\n",
      "        return np.trace(cm)/np.sum(cm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Testing the Network"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* How do we decide whether or not the network as learnt well?\n",
      "* We might see how well the trained network does on a **test set**\n",
      "* For classification problems we might use a **confusion matrix**, sometimes called a **prediction table** in statistics\n",
      "* The confusion matrix has the output classes in the columns and the targets in the rows\n",
      "* The cell at $(i,j)$ tells us how many input patterns in class were put in targets but class $j$ by our network"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "OR Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = [[0., 0.],\n",
      "     [0., 1.],\n",
      "     [1., 0.],\n",
      "     [1., 1.]]\n",
      "\n",
      "target = [0., 1., 1., 1.]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "or_clf = Perceptron(.25)\n",
      "or_clf.fit(X, target, max_iter=10, seed=1234)\n",
      "print \"Final output are: \\n\", or_clf.predict()\n",
      "print \"Final weights are: \\n\", or_clf.weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linear Separability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* What does the Perceptron actually compute?\n",
      "* As mentioned before, it computes a **decision boundary** (or **discriminant function**)\n",
      "* For the OR function, it tries to find a line that separates the data so that the neurons fire on one side and not the other\n",
      "* In 3D, we look for a plane, in higher dimensions, a hyperplane\n",
      "* Recall that the *inner product* of two vectors $a\\cdot b = \\|a\\|\\|b\\|\\cos \\theta$\n",
      "* When we find a boundary case for the Perceptron, we have a point where $x_1\\cdot w^{\\prime}=0$\n",
      "* Suppose we find another point where $x_2\\cdot w^{\\prime}=0$ then we have\n",
      "\n",
      "$$x_1\\cdot w^{\\prime} = x_2\\cdot w^{\\prime}$$\n",
      "$$(x_1-x_2)\\cdot w^{\\prime}=0$$\n",
      "\n",
      "* What do we know then from this last equation?\n",
      "* If such a line, $x_1 - x_2$, exists, then we can say that the problem is **linearly separable**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.scatter(*np.array(X)[1:].T, s=64)\n",
      "ax.scatter(*X[0], marker=\"+\", color=\"r\", s=64, lw=3);\n",
      "\n",
      "n = 100\n",
      "\n",
      "domain = np.linspace(0, 1, n)\n",
      "\n",
      "z1 = or_clf.predict(np.c_[domain, [0]*n])\n",
      "z2 = or_clf.predict(np.c_[[0]*n, domain])\n",
      "\n",
      "y1 = domain[np.where(z1)[0][0]]\n",
      "x1 = domain[np.where(z2)[0][0]]\n",
      "ax.plot([0, x1], [y1, 0], \"r-\")\n",
      "orth_weight = [x1/2, y1/2]\n",
      "\n",
      "direction = np.dot(orth_weight, or_clf.weights[:2])\n",
      "dx, dy = np.c_[np.sin(direction), np.cos(direction)][0]\n",
      "\n",
      "ax.arrow(orth_weight[0], orth_weight[1], dx/2, dy/2,\n",
      "    head_width=0.05, head_length=0.05, fc='k', ec='k', length_includes_head=True)\n",
      "ax.annotate(\"$w$\", orth_weight, (0,50), textcoords=\"offset points\",\n",
      "            fontsize=18);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "XOR Function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The xor function gives exclusive or"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array(X)\n",
      "zip(X, np.logical_xor(X[:,0], X[:,1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(8,6))\n",
      "ax.scatter(*X[1:3].T, s=64)\n",
      "ax.scatter(*X[[0,3]].T, marker=\"+\", color=\"r\", s=64, lw=3);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* As you can see from the graph, the XOR function is not linearly separable in 2D\n",
      "* This was demonstrated early on by Minksy and Papert in *Perceptrons* (1969)\n",
      "* It is one of the reasons that work on neural networks stagnated for a long time\n",
      "* In the next lecture, we will look at the solution to this problem"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xor_clf = Perceptron(.25)\n",
      "xor_clf.fit(X, [1, 0, 0, 1], 10)\n",
      "xor_clf.predict()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* An example of a **smoke test**\n",
      "  * Where there's smoke there's fire\n",
      "* A smoke test just makes sure that inputs run\n",
      "* It does not test for correctness"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_perceptron():\n",
      "    \"\"\" Run AND and XOR logic functions\"\"\"\n",
      "\n",
      "    # AND\n",
      "    inputs = [[0,0],\n",
      "              [0,1],\n",
      "              [1,0],\n",
      "              [1,1]]\n",
      "    \n",
      "    and_target = [0, 0, 0, 1]\n",
      "    \n",
      "    # XOR\n",
      "    xor_target = [0, 1, 1, 0]\n",
      "    \n",
      "    and_clf = Perceptron(.25)\n",
      "    and_clf.fit(inputs, and_target, 10)\n",
      "    and_clf.confusion_matrix(inputs, and_target, and_clf.weights)\n",
      "\n",
      "    xor_clf = Perceptron(.25)\n",
      "    xor_clf.fit(inputs, xor_target, 10)\n",
      "    xor_clf.confusion_matrix(inputs, xor_target, xor_clf.weights) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_perceptron()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example: The Pima Indian Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Information about this dataset can be found [here](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = (\"http://archive.ics.uci.edu/ml/machine-learning-databases\"\n",
      "       \"/pima-indians-diabetes/pima-indians-diabetes.data\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "#names = [\"num_pregnant\", \"plasma_glucose\", \"blood_press\", \n",
      "#         \"skin_fold\", \"insulin\", \"bmi\", \"pedigree\", \"age\",\n",
      "#         \"class\"]\n",
      "data = pandas.read_csv(url, header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classes = data[8]\n",
      "fig, ax = plt.subplots(figsize=(10,8))\n",
      "ax.plot(data.ix[classes==0, [0]], data.ix[classes==0, [1]], 'bo', label=\"No Diabetes\", alpha=.35)\n",
      "ax.plot(data.ix[classes==1, [0]], data.ix[classes==1, [1]], 'ro', label=\"Diabetes\", alpha=.35);\n",
      "ax.set_xlabel(\"Feature 1\")\n",
      "ax.set_ylabel(\"Feature 2\");\n",
      "ax.margins(.025)\n",
      "ax.legend(numpoints=1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try to use the Perceptron on this dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = data[range(8)]\n",
      "target = data[8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pima_clf = Perceptron(.25)\n",
      "pima_clf.fit(X, target, 100)\n",
      "pima_clf.confusion_matrix(X, target, pima_clf.weights)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This is unfair testing\n",
      "* What we really want to do is have the model learn on part of the dataset, called the **training set**\n",
      "* Then test it on the held-out, or out of sample, part, called the **test set**\n",
      "* We will talk more about this in the future, but for now use the even numbered observations as training and odd as test"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "train_X = data.values[::2,:8]\n",
      "test_X = data.values[1::2, :8]\n",
      "train_y = data.values[::2, 8]\n",
      "test_y = data.values[1::2, 8]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Instead of worrying much about training sets now\n",
      "* Let's look at how we can clean our data using pandas to improve prediction\n",
      "* Feature 0 is the number of pregnancies, let's *clip* this number at 8\n",
      "  - If you've been pregant 8 or more times, let's just set this as 8"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[0] = data[0].clip_upper(8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[0].max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Feature 7 is age\n",
      "* Let's **quantize** this into a set of ages 21-30, 31-40, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bins = range(21, data[7].max()+1, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bins"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[7].max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pandas.cut?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ages = pandas.cut(data[7], bins, include_lowest=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ages.labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[7] = ages.labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Another cleaning step that can improve results dramatically is **normalizing**, or **standardizing**, the data\n",
      "* One of the most common techniques for normalization is to ensure that you have variables with mean zero and unit variance\n",
      "* For some feature X\n",
      "\n",
      "$$z = \\frac{(X - \\mu)}{\\mbox{var}(X)}$$\n",
      "\n",
      "* You do this so that features that are much larger than others won't require a very small weight\n",
      "* **Note**, you should do this before splitting into testing and training sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[range(8)] = data[range(8)].apply(stats.zscore)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = data.values[:, :8]\n",
      "y = data.values[:, 8]\n",
      "pima_clf = Perceptron(.25)\n",
      "pima_clf.fit(X, y, 100)\n",
      "pima_clf.confusion_matrix(X, y, pima_clf.weights)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}